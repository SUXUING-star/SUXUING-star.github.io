(function(){var n={3937:function(n){n.exports={attributes:{title:"亚马逊评论文本简要的分析",date:"2024-11-17T00:00:00.000Z",summary:"这是数据分析部分的代码的描述和一些注释",coverImage:"11-19-4.jpg",pinned:!1},html:"<p>我做的数据分析主要分为两个部分，其实这个第一个部分是后面补充的。</p>\n<h1>自己简单爬取数据然后自己分析</h1>\n<p>这是油猴脚本写的js脚本，直接丢进油猴里边就完事，代码编写耗时可能20分钟以内</p>\n<pre><code class=\"language-javascript\">// ==UserScript==\n// @name         amazon-scraper1\n// @namespace    http://tampermonkey.net/\n// @version      2024-11-06\n// @description  try to take over the world!\n// @author       suxing\n// @match        https://www.amazon.com/*\n// @icon         https://www.google.com/s2/favicons?sz=64&amp;domain=amazon.com\n// @grant        none\n// ==/UserScript==\n\n\n\nfunction main(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;amazonbutton&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },2000)\n    }).then(()=&gt;{\n        let pageMain = document.querySelector(&quot;.sg-col-inner&quot;);\n        console.log(pageMain)\n        if(pageMain){\n            let button = document.createElement(&quot;button&quot;);\n            button.className=&quot;button-test&quot;\n            button.innerHTML = &quot;点击爬取json&quot;;\n            button.style.padding = &quot;10px 20px&quot;;\n            button.style.fontSize = &quot;16px&quot;;\n            button.style.backgroundColor = &quot;#4CAF50&quot;;\n            button.style.color = &quot;white&quot;;\n            button.style.border = &quot;none&quot;;\n            button.style.borderRadius = &quot;5px&quot;;\n            button.style.cursor = &quot;pointer&quot;;\n            button.style.boxShadow = &quot;0px 4px 6px rgba(0, 0, 0, 0.1)&quot;;\n            button.style.transition = &quot;background-color 0.3s&quot;;\n            button.onmouseover = function() {\n                button.style.backgroundColor = &quot;#45a049&quot;;\n            };\n            button.onmouseout = function() {\n                button.style.backgroundColor = &quot;#4CAF50&quot;;\n            };\n            button.onclick=()=&gt;{scrapefunc();}\n            var buttonContainer = document.createElement(&quot;div&quot;);\n            buttonContainer.style.display = &quot;flex&quot;;\n            buttonContainer.style.justifyContent = &quot;center&quot;;\n            buttonContainer.style.alignItems = &quot;center&quot;;\n            buttonContainer.style.height = &quot;100px&quot;; // 调整高度以便更好地居中\n            buttonContainer.appendChild(button);\n\n            // 在 pageMain 元素的上方插入按钮\n            pageMain.parentNode.insertBefore(buttonContainer, pageMain);\n        }\n    })\n}\n\nfunction exportjson(data){\n    const blob = new Blob([data], { type: 'application/json' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    const today = new Date();\n    const year = today.getFullYear();\n    const month = String(today.getMonth() + 1).padStart(2, '0');\n    const day = String(today.getDate()).padStart(2, '0');\n    const formattedDate = `${year}-${month}-${day}`;\n    a.download = `amazon-Products${formattedDate}.json`;\n    document.body.appendChild(a);\n    a.click();\n    document.body.removeChild(a);\n    URL.revokeObjectURL(url);\n}\nfunction scrapefunc(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;amazon-scraper test1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },700)\n    }).then(()=&gt;{\n        new Promise((resolve)=&gt;{\n            window.scrollTo({\n                top: 1000,\n                behavior: &quot;smooth&quot;\n            });\n            setTimeout(()=&gt;{\n                resolve();\n            },1000)\n        }).then(()=&gt;{\n            new Promise((resolve)=&gt;{\n                window.scrollTo({\n                    top: 4000,\n                    behavior: &quot;smooth&quot;\n                });\n                setTimeout(()=&gt;{\n                    resolve();\n                },1000)\n            }).then(()=&gt;{\n                new Promise((resolve)=&gt;{\n                    window.scrollTo({\n                        top: 8000,\n                        behavior: &quot;smooth&quot;\n                    });\n                    setTimeout(()=&gt;{\n                        resolve();\n                    },1000)\n                }).then(()=&gt;{\n                    new Promise((resolve)=&gt;{\n                        window.scrollTo({\n                            top: document.body.scrollHeight,\n                            behavior: &quot;smooth&quot;\n                        });\n                        setTimeout(()=&gt;{\n                            resolve();\n                        },700)\n                    }).then(()=&gt;{\n                        let productlist = [];\n                        document.querySelectorAll('div.a-section.a-spacing-small.puis-padding-left-small.puis-padding-right-small').forEach(container =&gt; {\n                            let product = {};\n\n                            // 标题和URL: 从商品标题容器获取\n                            try {\n                                const titleContainer = container.querySelector('[data-cy=&quot;title-recipe&quot;]');\n                                product.title = titleContainer.querySelector('.a-size-base-plus.a-color-base').textContent.trim();\n                                product.url = titleContainer.querySelector('a.a-link-normal').getAttribute('href');\n                            } catch (e) {\n                                product.title = '';\n                                product.url = '';\n                            }\n\n                            // 价格: 从价格容器获取\n                            try {\n                                const priceContainer = container.querySelector('[data-cy=&quot;price-recipe&quot;]');\n                                const priceElement = priceContainer.querySelector('.a-price .a-offscreen');\n                                product.price = priceElement ? priceElement.textContent.trim() : '';\n                            } catch (e) {\n                                product.price = '';\n                            }\n\n                            // 评分和评分数: 从评论容器获取\n                            try {\n                                const reviewsContainer = container.querySelector('[data-cy=&quot;reviews-block&quot;]');\n                                const ratingElement = reviewsContainer.querySelector('.a-icon-alt');\n                                product.rating = ratingElement ? ratingElement.textContent.trim() : '';\n\n                                const reviewsElement = reviewsContainer.querySelector('.rush-component .s-underline-text');\n                                product.ratingnum = reviewsElement ? reviewsElement.textContent.trim() : '';\n                            } catch (e) {\n                                product.rating = '';\n                                product.ratingnum = '';\n                            }\n\n                            if (product.title) {  // 只添加有标题的商品\n                                productlist.push(product);\n                            }\n                        });\n                        const productListJson = JSON.stringify(productlist, null, 2);\n                        console.log(productListJson);\n                        exportjson(productListJson);\n                        const nexturl=document.querySelector(&quot;.s-pagination-next&quot;).getAttribute(&quot;href&quot;)\n                        window.scrollTo({\n                            top: 0,\n                            behavior: &quot;smooth&quot;\n                        });\n                        new Promise((resolve)=&gt;{\n                            setTimeout(()=&gt;{\n                                resolve();\n                            },500)\n                        }).then(()=&gt;{\n                            location.href=nexturl\n                        })\n\n                    })\n                })\n            })\n        })\n    })\n}\n\n\n\n\n\nwindow.addEventListener(&quot;load&quot;,()=&gt;{\n    main();\n\n},false)\n</code></pre>\n<h2>python分析代码</h2>\n<pre><code class=\"language-python\"></code></pre>\n<h1>深度学习模型构建部分</h1>\n<h2>简要数据分析</h2>\n<pre><code class=\"language-python\"># 导入所需库\nfrom datasets import load_dataset\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer\n\n# 加载数据集，并取前2000条数据\ndataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\ndata = pd.DataFrame(dataset[&quot;full&quot;][:10000])  # 仅取2000条数据\nprint(data.head())\n\n# 1. 词云生成\nall_text = &quot; &quot;.join(data[&quot;text&quot;].fillna(&quot;&quot;))\nwordcloud = WordCloud(width=800, height=400, background_color=&quot;white&quot;).generate(all_text)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=&quot;bilinear&quot;)\nplt.axis(&quot;off&quot;)\nplt.title(&quot;Word Cloud of Amazon Reviews&quot;)\nplt.show()\n\n# 2. 用户画像分析\n# 评分分布\nplt.figure(figsize=(8, 5))\nsns.histplot(data[&quot;rating&quot;], bins=5, kde=True)\nplt.xlabel(&quot;Rating&quot;)\nplt.title(&quot;Distribution of Ratings&quot;)\nplt.show()\n\n# 验证购买分析\nverified_purchase_counts = data[&quot;verified_purchase&quot;].value_counts()\nplt.figure(figsize=(8, 5))\nsns.barplot(x=verified_purchase_counts.index, y=verified_purchase_counts.values)\nplt.xlabel(&quot;Verified Purchase&quot;)\nplt.ylabel(&quot;Count&quot;)\nplt.title(&quot;Distribution of Verified Purchases&quot;)\nplt.show()\n\n# 3. 时间序列分析：评论随时间的变化\ndata[&quot;timestamp&quot;] = pd.to_datetime(data[&quot;timestamp&quot;], unit=&quot;ms&quot;)  # 转换时间戳为日期格式\ndata.set_index(&quot;timestamp&quot;, inplace=True)\ndata[&quot;rating&quot;].resample(&quot;M&quot;).mean().plot(figsize=(12, 6))\nplt.title(&quot;Average Rating Over Time (Monthly)&quot;)\nplt.xlabel(&quot;Time&quot;)\nplt.xlim(pd.Timestamp(&quot;2012-01-01&quot;), pd.Timestamp(&quot;2023-12-31&quot;))\nplt.ylabel(&quot;Average Rating&quot;)\nplt.show()\n\n# 4. 评论字数分析\ndata[&quot;review_length&quot;] = data[&quot;text&quot;].apply(lambda x: len(str(x).split()))  # 计算每条评论的词数\ndata[&quot;review_length&quot;].resample(&quot;M&quot;).mean().plot(figsize=(12, 6))\nplt.title(&quot;Average Review Length Over Time (Monthly)&quot;)\nplt.xlabel(&quot;Time&quot;)\nplt.xlim(pd.Timestamp(&quot;2012-01-01&quot;), pd.Timestamp(&quot;2023-12-31&quot;))\nplt.ylabel(&quot;Average Review Length&quot;)\nplt.show()\n\n\n\n\n\n</code></pre>\n"}},9317:function(n){n.exports={attributes:{title:"YOLO计算机视觉&目标检测模型",date:"2024-11-01T00:00:00.000Z",summary:"在尝试的学习CV技术",coverImage:"3.png",pinned:!1},html:"<h1>开始学点DL技术！</h1>\n<h2>上点干货的算法</h2>\n<p>你以为的YOLO ：<strong>you only look once</strong></p>\n<p>我以为的YOLO： <strong>YOU ONLY LIVE ONCE</strong></p>\n<p><strong>珍爱生命，远离深度学习啊！！</strong></p>\n<h2>研究一下YOLO的模型结构</h2>\n<p>以YOLOv3为例</p>\n<ol>\n<li>\n<p>输入:  模型的输入是一张图片。</p>\n</li>\n<li>\n<p>骨干网络 (Backbone Network):  用于提取图像特征。YOLOv3 使用 Darknet-53 作为骨干网络，它是一个 53 层的卷积神经网络，具有残差连接，可以有效地提取图像特征。</p>\n</li>\n<li>\n<p>特征金字塔网络 (Feature Pyramid Network, FPN):  用于在不同尺度上检测目标。FPN 将骨干网络提取的不同层级的特征图进行融合，生成多个尺度的特征图，从而可以检测不同大小的目标。</p>\n</li>\n<li>\n<p>检测头 (Detection Head):  用于预测目标的类别和位置。YOLOv3 的检测头包含三个分支，分别对应三个不同的尺度。每个分支都包含一系列卷积层，最终输出一个三维张量，其中包含每个网格单元的预测信息。</p>\n</li>\n</ol>\n<p>将输入图像分成 S x S 个网格单元。\n每个网格单元负责预测 B 个边界框和 C 个类别概率。\n每个边界框包含 5 个预测值：x，y，w，h 和置信度。\n(x, y) 是边界框中心相对于网格单元的坐标。\n(w, h) 是边界框的宽度和高度相对于整张图片的比例。\n置信度表示边界框包含目标的概率以及边界框的准确度。\n每个网格单元还会预测 C 个类别概率，表示该网格单元包含某个类别的目标的概率。</p>\n"}},2879:function(n){n.exports={attributes:{title:"用亚马逊评论数据，玩一下生成式模型",date:"2024-11-01T00:00:00.000Z",summary:"学习一下生成式模型",coverImage:"11-19-2.jpg",pinned:!1},html:"<h1>T5预训练模型上做微调</h1>\n<pre><code class=\"language-python\"># 导入所需库\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom transformers import (\n    T5ForConditionalGeneration, T5Tokenizer,\n    get_linear_schedule_with_warmup\n)\nimport gc\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport pandas as pd\nimport copy\nimport os\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport re\n\n# 下载必要的NLTK数据\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.corpus import wordnet\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed()\n# 数据增强方法\nclass TextAugmenter:\n    @staticmethod\n    def synonym_replacement(text, n=1):\n        words = word_tokenize(text)\n        new_words = words.copy()\n        random_word_list = list(set([word for word in words if word.isalnum()]))\n        n = min(n, len(random_word_list))\n        \n        for _ in range(n):\n            random_word = random.choice(random_word_list)\n            synonyms = []\n            for syn in wordnet.synsets(random_word):\n                for lemma in syn.lemmas():\n                    synonyms.append(lemma.name())\n            if len(synonyms) &gt; 0:\n                synonym = random.choice(list(set(synonyms)))\n                random_idx = random.randint(0, len(words) - 1)\n                new_words[random_idx] = synonym\n        \n        return ' '.join(new_words)\n\n    @staticmethod\n    def random_deletion(text, p=0.1):\n        words = word_tokenize(text)\n        if len(words) == 1:\n            return text\n        \n        new_words = []\n        for word in words:\n            if random.random() &gt; p:\n                new_words.append(word)\n        \n        if len(new_words) == 0:\n            rand_int = random.randint(0, len(words)-1)\n            new_words.append(words[rand_int])\n            \n        return ' '.join(new_words)\n\n    @staticmethod\n    def random_swap(text, n=1):\n        words = word_tokenize(text)\n        new_words = words.copy()\n        for _ in range(n):\n            if len(new_words) &gt;= 2:\n                idx1, idx2 = random.sample(range(len(new_words)), 2)\n                new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n        return ' '.join(new_words)\n\nclass ReviewGenerationDataset(Dataset):\n    def __init__(self, texts, ratings, tokenizer, max_len=128, augment=False):\n        # 将Series转换为list以避免索引问题\n        self.texts = texts.tolist()\n        self.ratings = ratings.tolist()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.augment = augment\n        self.augmenter = TextAugmenter()\n\n    def __len__(self):\n        return len(self.texts)\n\n    def augment_text(self, text):\n        augmentation_ops = [\n            (self.augmenter.synonym_replacement, {'n': 1}),\n            (self.augmenter.random_deletion, {'p': 0.1}),\n            (self.augmenter.random_swap, {'n': 1})\n        ]\n        op, params = random.choice(augmentation_ops)\n        return op(text, **params)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        rating = self.ratings[idx]\n        \n        if self.augment and random.random() &lt; 0.3:\n            text = self.augment_text(text)\n        \n        prompt = f&quot;Generate a {rating}-star review:&quot;\n        \n        prompt_encoding = self.tokenizer(\n            prompt,\n            max_length=32,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        target_encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': prompt_encoding['input_ids'].squeeze(),\n            'attention_mask': prompt_encoding['attention_mask'].squeeze(),\n            'labels': target_encoding['input_ids'].squeeze(),\n            'decoder_attention_mask': target_encoding['attention_mask'].squeeze()\n        }\n\n\nclass ReviewGenerator(nn.Module):\n    def __init__(self, model_name=&quot;t5-base&quot;):\n        super().__init__()\n        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n        # 启用梯度检查点以减少内存使用\n        self.model.gradient_checkpointing_enable()\n        \n    def forward(self, input_ids, attention_mask, labels=None, decoder_attention_mask=None):\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels,\n            decoder_attention_mask=decoder_attention_mask\n        )\n        return outputs\n\n\ndef prepare_data(batch_size=8):  # 减小batch size以降低内存使用\n    # 加载数据集\n    dataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\n    data = pd.DataFrame(dataset[&quot;full&quot;][:200000])\n    \n    # 数据清洗\n    data['text'] = data['text'].fillna(&quot;&quot;).astype(str)\n    data = data[data['text'].str.len() &gt; 10]\n    data = data.reset_index(drop=True)\n    \n    # 划分训练集和验证集\n    train_texts, val_texts, train_ratings, val_ratings = train_test_split(\n        data['text'], data['rating'],\n        test_size=0.1,\n        random_state=42\n    )\n    \n    tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=512)  # 减小max_length\n    \n    train_dataset = ReviewGenerationDataset(train_texts, train_ratings, tokenizer, augment=True)\n    val_dataset = ReviewGenerationDataset(val_texts, val_ratings, tokenizer, augment=False)\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        drop_last=True,\n        pin_memory=True  # 启用pin_memory加速数据传输\n    )\n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=batch_size,\n        drop_last=True,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader, tokenizer\n\n\ndef train_generator(model, train_loader, val_loader, epochs=5, save_dir=&quot;generator_checkpoints&quot;, \n                   gradient_accumulation_steps=4):\n    # 创建输出目录\n    os.makedirs(save_dir, exist_ok=True)\n    os.makedirs(&quot;generative-output&quot;, exist_ok=True)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n    num_training_steps = (len(train_loader) // gradient_accumulation_steps) * epochs\n    num_warmup_steps = num_training_steps // 10\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    \n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        model.train()\n        total_train_loss = 0\n        optimizer.zero_grad()\n        \n        with tqdm(total=len(train_loader), desc=f&quot;Epoch {epoch+1}/{epochs}&quot;) as pbar:\n            for batch_idx, batch in enumerate(train_loader):\n                # 将数据移动到GPU\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n                \n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                    decoder_attention_mask=decoder_attention_mask\n                )\n                \n                loss = outputs.loss / gradient_accumulation_steps\n                total_train_loss += loss.item() * gradient_accumulation_steps\n                \n                loss.backward()\n                \n                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                    optimizer.step()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    \n                    # 清理缓存\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                \n                pbar.update(1)\n                pbar.set_postfix({\n                    'train_loss': f'{loss.item() * gradient_accumulation_steps:.4f}'\n                })\n        \n        # 处理最后一个不完整的累积步\n        if len(train_loader) % gradient_accumulation_steps != 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n        \n        # 评估阶段\n        model.eval()\n        total_val_loss = 0\n        \n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                decoder_attention_mask = batch['decoder_attention_mask'].to(device)\n                \n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                    decoder_attention_mask=decoder_attention_mask\n                )\n                \n                total_val_loss += outputs.loss.item()\n        \n        avg_train_loss = total_train_loss / len(train_loader)\n        avg_val_loss = total_val_loss / len(val_loader)\n        \n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        \n        print(f&quot;\\nEpoch {epoch+1}&quot;)\n        print(f&quot;Average training loss: {avg_train_loss:.4f}&quot;)\n        print(f&quot;Average validation loss: {avg_val_loss:.4f}&quot;)\n        \n        if avg_val_loss &lt; best_val_loss:\n            best_val_loss = avg_val_loss\n            torch.save(model.state_dict(), f&quot;{save_dir}/best_model.pth&quot;)\n            print(&quot;Saved new best model!&quot;)\n        \n        # 绘制并保存loss曲线\n        plt.figure(figsize=(10, 6))\n        plt.plot(range(1, epoch + 2), train_losses, label='Training Loss')\n        plt.plot(range(1, epoch + 2), val_losses, label='Validation Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.title('Training and Validation Loss')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig('generative-output/loss_curve.png')\n        plt.close()\n        \n        # 强制进行垃圾回收\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n\ndef generate_review(model, tokenizer, rating, max_length=150):\n    model.eval()\n    prompt = f&quot;Generate a {rating}-star review:&quot;\n    \n    inputs = tokenizer(\n        prompt,\n        return_tensors=&quot;pt&quot;,\n        max_length=32,\n        padding=True,\n        truncation=True\n    ).to(device)\n    \n    with torch.no_grad():\n        outputs = model.model.generate(\n            input_ids=inputs[&quot;input_ids&quot;],\n            attention_mask=inputs[&quot;attention_mask&quot;],\n            max_length=max_length,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            top_k=50,\n            top_p=0.95,\n            temperature=0.7,\n            do_sample=True\n        )\n    \n    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated_text\n\n\ndef main():\n    # 清理GPU内存\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        \n    train_loader, val_loader, tokenizer = prepare_data()\n    model = ReviewGenerator().to(device)\n    train_generator(model, train_loader, val_loader)\n    \n    # 生成示例评论\n    model.load_state_dict(torch.load(&quot;generator_checkpoints/best_model.pth&quot;))\n    \n    for rating in [1, 3, 5]:\n        print(f&quot;\\nGenerated {rating}-star review:&quot;)\n        review = generate_review(model, tokenizer, rating)\n        print(review)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n"}},3352:function(n){n.exports={attributes:{title:"亚马逊评论数据做文本情感分类模型",date:"2024-11-18T00:00:00.000Z",summary:"在尝试的学习NLP技术",coverImage:"11-19-3.jpg",pinned:!0},html:"<p>模型大致分为</p>\n<p>LSTM模型</p>\n<p>LSTM带注意力机制</p>\n<p>微调BERT预训练模型，写在另外一篇文章了</p>\n<p>微调T5生成式模型（补充），这个也写在另外一篇文章了</p>\n<h1>LSTM模型</h1>\n<h2>模型1</h2>\n<p>模型结构可以概括为以下几个步骤：</p>\n<p>BERT词嵌入: 使用预训练的BERT模型对文本进行编码，将每个词转换为一个高维向量表示。BERT能够捕捉词语的上下文信息，从而生成更准确的词嵌入。</p>\n<p>LSTM层: 将BERT词嵌入序列输入到LSTM层中。LSTM能够捕捉序列数据中的长期依赖关系，从而更好地理解评论的整体情感。</p>\n<p>全连接层: LSTM层的输出经过一个全连接层，将高维的隐藏状态映射到一个数值。</p>\n<p>Tanh激活函数: 全连接层的输出经过Tanh激活函数，将预测评分限制在-1到1之间。</p>\n<pre><code class=\"language-python\"># 导入所需库\nfrom datasets import load_dataset #datasets用于导入huggingface上的数据集\nfrom wordcloud import WordCloud #这是一个词云库\nimport matplotlib.pyplot as plt #画图库\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split #用于划分数据集\nimport torch \nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer 使用Bert 分词器\n\n\n# 导入所需库\nfrom datasets import load_dataset  # datasets用于导入huggingface上的数据集\nfrom wordcloud import WordCloud  # 这是一个词云库\nimport matplotlib.pyplot as plt  # 画图库\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split  # 用于划分数据集\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer  # 使用Bert 分词器\n\n\n# 加载数据集，并取前10000条数据\ndataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\ndata = pd.DataFrame(dataset[&quot;full&quot;][:10000])  # 仅取10000条数据\nprint(data.head())\n\n# 设置Matplotlib的字体参数\nplt.rcParams['font.family'] = 'SimHei'  # 替换为你选择的字体\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n# 初始化BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmax_len = 128  # 设置最大序列长度\n\n\n# 定义数据集类\nclass ReviewDataset(Dataset):\n    def __init__(self, texts, ratings):\n        self.texts = texts\n        self.ratings = ratings\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # 使用BertTokenizer对文本进行编码\n        encoded = tokenizer.encode_plus(\n            self.texts[idx],\n            add_special_tokens=True,  # 添加特殊标记\n            max_length=max_len,  # 设置最大长度\n            padding=&quot;max_length&quot;,  # 使用最大长度进行填充\n            truncation=True,  # 超过最大长度进行截断\n            return_tensors=&quot;pt&quot;,  # 返回pytorch张量\n        )\n        input_ids = encoded[&quot;input_ids&quot;].squeeze()  # 获取input_ids\n        attention_mask = encoded[&quot;attention_mask&quot;].squeeze()  # 获取attention_mask\n        rating = torch.tensor(self.ratings[idx], dtype=torch.float)  # 将评分转换为张量\n        return input_ids, attention_mask, rating\n\n\n# 划分训练集和验证集\ntrain_texts, val_texts, train_ratings, val_ratings = train_test_split(\n    data[&quot;text&quot;].fillna(&quot;&quot;), data[&quot;rating&quot;], test_size=0.2, random_state=42\n)\n# 创建训练集和验证集\ntrain_dataset = ReviewDataset(train_texts.tolist(), train_ratings.tolist())\nval_dataset = ReviewDataset(val_texts.tolist(), val_ratings.tolist())\n# 创建DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n\n# 定义LSTM模型\nclass ReviewLSTM(nn.Module):\n    def __init__(self, embedding_dim=64, hidden_dim=128, output_dim=1):\n        super(ReviewLSTM, self).__init__()\n        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)  # 词嵌入层\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)  # LSTM层\n        self.fc = nn.Linear(hidden_dim, output_dim)  # 全连接层\n        self.tanh = nn.Tanh()  # tanh激活函数\n\n    def forward(self, input_ids, attention_mask):\n        embedded = self.embedding(input_ids)  # 获取词嵌入向量\n        _, (hidden, _) = self.lstm(embedded)  # 通过LSTM层\n        output = self.tanh(self.fc(hidden[-1]))  # 通过全连接层并使用tanh激活函数\n        return output\n\n\n# 初始化模型、损失函数和优化器\nmodel = ReviewLSTM()\ncriterion = nn.MSELoss()  # 使用均方误差作为损失函数\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # 使用Adam优化器\n\n\n# 定义训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20, patience=3):\n    train_losses, val_losses = [], []  # 用于存储训练和验证损失\n    best_val_loss = float(&quot;inf&quot;)  # 初始化最佳验证损失\n    patience_counter = 0  # 初始化早停计数器\n\n    for epoch in range(epochs):\n        model.train()  # 设置模型为训练模式\n        running_loss = 0\n        for input_ids, attention_mask, ratings in train_loader:\n            optimizer.zero_grad()  # 清空梯度\n            outputs = model(input_ids, attention_mask).squeeze()  # 获取模型输出\n            loss = criterion(outputs, ratings)  # 计算损失\n            loss.backward()  # 反向传播\n            optimizer.step()  # 更新模型参数\n            running_loss += loss.item()  # 累加损失\n\n        train_loss = running_loss / len(train_loader)  # 计算平均训练损失\n        train_losses.append(train_loss)  # 存储训练损失\n\n        # 验证模型\n        model.eval()  # 设置模型为评估模式\n        val_loss = 0\n        with torch.no_grad():  # 不计算梯度\n            for input_ids, attention_mask, ratings in val_loader:\n                outputs = model(input_ids, attention_mask).squeeze()  # 获取模型输出\n                loss = criterion(outputs, ratings)  # 计算损失\n                val_loss += loss.item()  # 累加损失\n\n        val_loss /= len(val_loader)  # 计算平均验证损失\n        val_losses.append(val_loss)  # 存储验证损失\n\n        print(f&quot;Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}&quot;)\n\n        # 早停机制\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                print(&quot;早停法：验证损失未改善，提前停止训练。&quot;)\n                break\n\n    # 绘制损失曲线\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label=&quot;训练损失&quot;)\n    plt.plot(val_losses, label=&quot;验证损失&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;损失&quot;)\n    plt.legend()\n    plt.show()\n# 开始训练\ntrain_model(model, train_loader, val_loader, criterion, optimizer)\n</code></pre>\n<h2>模型2</h2>\n<pre><code class=\"language-python\">from sklearn.metrics.pairwise import cosine_similarity  # 用于计算余弦相似度\nfrom transformers import BertModel, BertTokenizer  # 用于加载BERT模型和分词器\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset  # 用于创建数据加载器和数据集\nfrom sklearn.model_selection import train_test_split  # 用于划分数据集\nimport matplotlib.pyplot as plt  # 用于绘图\nimport pandas as pd  # 用于数据处理\nimport seaborn as sns  # 用于绘制热力图\nfrom datasets import load_dataset  # 用于加载Hugging Face数据集\nfrom sklearn.metrics import accuracy_score  # 用于计算准确率\n\n# 设置Matplotlib的字体参数\nplt.rcParams['font.family'] = 'SimHei'  # 替换为你选择的字体\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\n# 检查CUDA是否可用\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f&quot;Using device: {device}&quot;)\n\n# 加载数据集，并取前10000条数据\ndataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\ndata = pd.DataFrame(dataset[&quot;full&quot;][:10000])  # 仅取10000条数据\n\n# 设置BERT的分词器\ntokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmax_len = 128  # 设置最大序列长度\n\n# 定义数据集类\nclass ReviewDataset(Dataset):\n    def __init__(self, texts, ratings):\n        self.texts = texts\n        # 将评分转换为类别索引 (1-5星 -&gt; 0-4)\n        self.ratings = [int(rating) - 1 for rating in ratings]  \n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # 使用BertTokenizer对文本进行编码\n        encoded = tokenizer.encode_plus(\n            self.texts[idx],\n            add_special_tokens=True,  # 添加特殊标记\n            max_length=max_len,  # 设置最大长度\n            padding=&quot;max_length&quot;,  # 使用最大长度进行填充\n            truncation=True,  # 超过最大长度进行截断\n            return_tensors=&quot;pt&quot;,  # 返回pytorch张量\n        )\n        input_ids = encoded[&quot;input_ids&quot;].squeeze()  # 获取input_ids\n        attention_mask = encoded[&quot;attention_mask&quot;].squeeze()  # 获取attention_mask\n        # 使用long类型以适应分类任务\n        rating = torch.tensor(self.ratings[idx], dtype=torch.long)  \n        return input_ids, attention_mask, rating\n\n# 划分训练集和验证集\ntrain_texts, val_texts, train_ratings, val_ratings = train_test_split(\n    data[&quot;text&quot;].fillna(&quot;&quot;), data[&quot;rating&quot;], test_size=0.2, random_state=42\n)\n# 创建训练集和验证集\ntrain_dataset = ReviewDataset(train_texts.tolist(), train_ratings.tolist())\nval_dataset = ReviewDataset(val_texts.tolist(), val_ratings.tolist())\n# 创建DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# 修改模型结构，将LSTM层数调整为两层\nclass ReviewLSTM(nn.Module):\n    def __init__(self, embedding_dim=128, hidden_dim=256, output_dim=5, dropout_rate=0.3):  # embedding_dim增至128，hidden_dim增至256\n        super(ReviewLSTM, self).__init__()\n        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)\n\n        # 双层LSTM\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, dropout=dropout_rate, batch_first=True)\n\n        # 增加两层全连接层并加入Dropout\n        self.fc1 = nn.Linear(hidden_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, output_dim)\n\n        # Dropout层\n        self.dropout = nn.Dropout(dropout_rate)\n        self.softmax = nn.Softmax(dim=1)\n\n        # 权重初始化\n        self._init_weights()\n\n    def _init_weights(self):\n        # 初始化模型权重\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LSTM):\n                for name, param in m.named_parameters():\n                    if 'weight' in name:\n                        nn.init.xavier_uniform_(param)\n                    elif 'bias' in name:\n                        nn.init.zeros_(param)\n\n    def forward(self, input_ids, attention_mask):\n        embedded = self.embedding(input_ids)  # 获取词嵌入向量\n        lstm_out, (hidden, _) = self.lstm(embedded)  # 通过LSTM层\n\n        # 使用最后一层LSTM的隐藏状态并通过全连接层\n        hidden = self.dropout(hidden[-1])\n        x = self.fc1(hidden)\n        x = self.dropout(nn.ReLU()(x))\n        x = self.fc2(x)\n        x = self.dropout(nn.ReLU()(x))\n        x = self.fc3(x)\n\n        return self.softmax(x)  # 返回softmax后的概率分布\n\n\n# 初始化模型、损失函数和优化器\nmodel = ReviewLSTM().to(device)  # 将模型移动到 CUDA\ncriterion = nn.CrossEntropyLoss()  # 使用交叉熵损失\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # 使用Adam优化器\n\n# 修改训练函数，增加准确度计算\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20, patience=3):\n    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []  # 用于存储训练和验证损失及准确率\n    best_val_loss = float(&quot;inf&quot;)  # 初始化最佳验证损失\n    patience_counter = 0  # 初始化早停计数器\n\n    for epoch in range(epochs):\n        model.train()  # 设置模型为训练模式\n        running_loss = 0\n        train_predictions, train_labels = [], []  # 用于存储训练集的预测结果和真实标签\n\n        for input_ids, attention_mask, ratings in train_loader:\n            # 将数据移动到GPU\n            input_ids, attention_mask, ratings = input_ids.to(device), attention_mask.to(device), ratings.to(device)  \n            optimizer.zero_grad()  # 清空梯度\n            outputs = model(input_ids, attention_mask)  # 获取模型输出\n            loss = criterion(outputs, ratings)  # 计算损失\n            loss.backward()  # 反向传播\n            optimizer.step()  # 更新模型参数\n            running_loss += loss.item()  # 累加损失\n\n            # 记录预测结果和真实标签\n            train_predictions.extend(outputs.argmax(dim=1).cpu().numpy())  \n            train_labels.extend(ratings.cpu().numpy())  \n\n        train_loss = running_loss / len(train_loader)  # 计算平均训练损失\n        train_accuracy = accuracy_score(train_labels, train_predictions)  # 计算训练准确率\n        train_losses.append(train_loss)  # 存储训练损失\n        train_accuracies.append(train_accuracy)  # 存储训练准确率\n\n        # 验证模型\n        model.eval()  # 设置模型为评估模式\n        val_loss = 0\n        val_predictions, val_labels = [], []  # 用于存储验证集的预测结果和真实标签\n\n        with torch.no_grad():  # 不计算梯度\n            for input_ids, attention_mask, ratings in val_loader:\n                # 将数据移动到GPU\n                input_ids, attention_mask, ratings = input_ids.to(device), attention_mask.to(device), ratings.to(device)  \n                outputs = model(input_ids, attention_mask)  # 获取模型输出\n                loss = criterion(outputs, ratings)  # 计算损失\n                val_loss += loss.item()  # 累加损失\n\n                # 记录预测结果和真实标签\n                val_predictions.extend(outputs.argmax(dim=1).cpu().numpy()) \n                val_labels.extend(ratings.cpu().numpy()) \n\n        val_loss /= len(val_loader)  # 计算平均验证损失\n        val_accuracy = accuracy_score(val_labels, val_predictions)  # 计算验证准确率\n        val_losses.append(val_loss)  # 存储验证损失\n        val_accuracies.append(val_accuracy)  # 存储验证准确率\n\n        # 打印训练和验证信息\n        print(f&quot;Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, &quot;\n              f&quot;Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}&quot;)\n\n        # 早停机制\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                print(&quot;早停法：验证损失未改善，提前停止训练。&quot;)\n                break\n\n    # 绘制损失和准确度曲线\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label=&quot;训练损失&quot;)\n    plt.plot(val_losses, label=&quot;验证损失&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;损失&quot;)\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accuracies, label=&quot;训练准确度&quot;)\n    plt.plot(val_accuracies, label=&quot;验证准确度&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;准确度&quot;)\n    plt.legend()\n    plt.show()\n\n# 开始训练\ntrain_model(model, train_loader, val_loader, criterion, optimizer)\n\n# 导入混淆矩阵和seaborn库\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# 生成混淆矩阵\ndef plot_confusion_matrix(model, val_loader):\n    model.eval()  # 设置模型为评估模式\n    val_predictions, val_labels = [], []  # 用于存储验证集的预测结果和真实标签\n\n    with torch.no_grad():  # 不计算梯度\n        for input_ids, attention_mask, ratings in val_loader:\n            # 将数据移动到GPU\n            input_ids, attention_mask, ratings = input_ids.to(device), attention_mask.to(device), ratings.to(device)\n            outputs = model(input_ids, attention_mask)  # 获取模型输出\n            predictions = outputs.argmax(dim=1).cpu().numpy()  # 获取预测结果\n            val_predictions.extend(predictions)  # 存储预测结果\n            val_labels.extend(ratings.cpu().numpy())  # 存储真实标签\n\n    # 计算混淆矩阵\n    cm = confusion_matrix(val_labels, val_predictions)\n    plt.figure(figsize=(8, 6))\n    # 绘制热力图\n    sns.heatmap(cm, annot=True, fmt='d', cmap=&quot;Blues&quot;, xticklabels=range(1, 6), yticklabels=range(1, 6))  \n    plt.xlabel(&quot;Predicted Labels&quot;)\n    plt.ylabel(&quot;True Labels&quot;)\n    plt.title(&quot;Confusion Matrix&quot;)\n    plt.show()\n\n# 生成并绘制混淆矩阵\nplot_confusion_matrix(model, val_loader)\n</code></pre>\n<p>主要改进：</p>\n<p>双层LSTM: 使用两层LSTM来提取更深层次的文本特征。\n更宽的网络: 增加了embedding_dim和hidden_dim的维度，以及两层全连接层，使模型容量更大。\nDropout: 在LSTM层和全连接层之间加入Dropout，防止过拟合。\n权重初始化: 使用Xavier均匀分布初始化线性层的权重，用零初始化偏置。\n交叉熵损失: 使用nn.CrossEntropyLoss()作为损失函数，适用于多分类任务。\nSoftmax: 使用Softmax将模型输出转换为概率分布，表示每个星级的预测概率。\n准确率: 在训练过程中计算并输出训练集和验证集的准确率。\n混淆矩阵: 绘制混淆矩阵，可视化模型在不同类别上的预测效果。\n模型结构可以概括为以下几个步骤：</p>\n<p>BERT词嵌入: 使用预训练的BERT模型对文本进行编码，将每个词转换为一个高维向量表示。</p>\n<p>双层LSTM: 将BERT词嵌入序列输入到两层LSTM中，提取更深层次的文本特征。</p>\n<p>全连接层: LSTM层的输出经过三层全连接层，将高维的隐藏状态映射到五个类别（对应五种星级）。</p>\n<p>Softmax: 全连接层的输出经过Softmax激活函数，得到每个类别的预测概率。</p>\n<h1>带注意力机制的LSTM</h1>\n<h2>模型1</h2>\n<pre><code class=\"language-python\"># 导入所需库\nfrom datasets import load_dataset  # 用于加载Hugging Face数据集\nimport matplotlib.pyplot as plt  # 用于绘图\nimport pandas as pd  # 用于数据处理\nfrom sklearn.model_selection import train_test_split  # 用于划分数据集\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset  # 用于创建数据加载器和数据集\nfrom transformers import BertTokenizer  # 用于加载BERT分词器\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # 用于评估模型性能\nimport seaborn as sns  # 用于绘制热力图\n\n# 检查CUDA是否可用\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f&quot;Using device: {device}&quot;)\n\n# 加载数据集，并取前20000条数据\ndataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\ndata = pd.DataFrame(dataset[&quot;full&quot;][:20000])\nprint(data.head())\n\n# 设置Matplotlib的字体参数\nplt.rcParams['font.family'] = 'SimHei'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 初始化BERT分词器\ntokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmax_len = 128  # 设置最大序列长度\n\n# 定义数据集类\nclass ReviewDataset(Dataset):\n    def __init__(self, texts, ratings):\n        self.texts = texts\n        self.ratings = ratings\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # 使用BertTokenizer对文本进行编码\n        encoded = tokenizer.encode_plus(\n            self.texts[idx],\n            add_special_tokens=True,  # 添加特殊标记\n            max_length=max_len,  # 设置最大长度\n            padding=&quot;max_length&quot;,  # 使用最大长度进行填充\n            truncation=True,  # 超过最大长度进行截断\n            return_tensors=&quot;pt&quot;,  # 返回pytorch张量\n        )\n        input_ids = encoded[&quot;input_ids&quot;].squeeze()  # 获取input_ids\n        attention_mask = encoded[&quot;attention_mask&quot;].squeeze()  # 获取attention_mask\n        # 将评分转换为类别索引（1-5 → 0-4）\n        rating = torch.tensor(int(self.ratings[idx]) - 1, dtype=torch.long)  \n        return input_ids, attention_mask, rating\n\n# 增强版LSTM分类模型\nclass EnhancedReviewClassifier(nn.Module):\n    def __init__(self, embedding_dim=256, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True, num_classes=5):\n        super(EnhancedReviewClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)  # 词嵌入层\n        self.dropout = nn.Dropout(dropout)  # Dropout层\n\n        # 双向LSTM\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=num_layers,  # LSTM层数\n            batch_first=True,\n            bidirectional=bidirectional,  # 是否双向\n            dropout=dropout if num_layers &gt; 1 else 0  # LSTM层之间添加Dropout\n        )\n\n        # 注意力机制\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim),  # 全连接层\n            nn.Tanh(),  # Tanh激活函数\n            nn.Linear(hidden_dim, 1),  # 全连接层\n            nn.Softmax(dim=1)  # Softmax激活函数，计算注意力权重\n        )\n\n        # 全连接层\n        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim  # 根据LSTM是否双向确定输入维度\n        self.fc1 = nn.Linear(fc_input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)  # 输出改为5个类别\n\n        self.relu = nn.ReLU()  # ReLU激活函数\n        self.softmax = nn.Softmax(dim=1)  # 添加softmax层，用于输出概率分布\n\n    def forward(self, input_ids, attention_mask):\n        # 词嵌入\n        embedded = self.embedding(input_ids)\n        embedded = self.dropout(embedded)\n\n        # LSTM层\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n\n        # 注意力机制\n        attention_weights = self.attention(lstm_out)  # 计算注意力权重\n        attention_output = torch.sum(attention_weights * lstm_out, dim=1)  # 对LSTM输出进行加权求和\n\n        # 全连接层\n        x = self.dropout(attention_output)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)  # 不在这里应用softmax，因为CrossEntropyLoss已包含\n\n        return x\n\n# 数据预处理和加载\ntrain_texts, val_texts, train_ratings, val_ratings = train_test_split(\n    data[&quot;text&quot;].fillna(&quot;&quot;), data[&quot;rating&quot;], test_size=0.2, random_state=42\n)\n\ntrain_dataset = ReviewDataset(train_texts.tolist(), train_ratings.tolist())\nval_dataset = ReviewDataset(val_texts.tolist(), val_ratings.tolist())\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# 初始化模型、损失函数和优化器\nmodel = EnhancedReviewClassifier().to(device)\ncriterion = nn.CrossEntropyLoss()  # 改用CrossEntropyLoss，适用于多分类任务\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)  # 使用AdamW优化器\n# 使用ReduceLROnPlateau学习率调度器，当验证损失停止下降时降低学习率\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)  \n\n# 绘制混淆矩阵\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # 绘制热力图\n    plt.title('混淆矩阵')\n    plt.xlabel('预测类别')\n    plt.ylabel('真实类别')\n    plt.show()\n\n# 训练模型\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, patience=5):\n    train_losses, val_losses = [], []  # 用于存储训练和验证损失\n    train_accs, val_accs = [], []  # 用于存储训练和验证准确率\n    best_val_loss = float(&quot;inf&quot;)  # 初始化最佳验证损失\n    patience_counter = 0  # 初始化早停计数器\n\n    for epoch in range(epochs):\n        # 训练阶段\n        model.train()  # 设置模型为训练模式\n        running_loss = 0\n        train_preds = []  # 用于存储训练集的预测结果\n        train_true = []  # 用于存储训练集的真实标签\n\n        for input_ids, attention_mask, ratings in train_loader:\n            input_ids = input_ids.to(device)  # 将数据移动到GPU\n            attention_mask = attention_mask.to(device)  # 将数据移动到GPU\n            ratings = ratings.to(device)  # 将数据移动到GPU\n\n            optimizer.zero_grad()  # 清空梯度\n            outputs = model(input_ids, attention_mask)  # 获取模型输出\n            loss = criterion(outputs, ratings)  # 计算损失\n            loss.backward()  # 反向传播\n\n            # 梯度裁剪，防止梯度爆炸\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n\n            optimizer.step()  # 更新模型参数\n            running_loss += loss.item()  # 累加损失\n\n            # 收集预测结果\n            _, predicted = torch.max(outputs.data, 1)  # 获取预测类别\n            train_preds.extend(predicted.cpu().numpy())  # 存储预测结果\n            train_true.extend(ratings.cpu().numpy())  # 存储真实标签\n\n        train_loss = running_loss / len(train_loader)  # 计算平均训练损失\n        train_acc = accuracy_score(train_true, train_preds)  # 计算训练准确率\n        train_losses.append(train_loss)  # 存储训练损失\n        train_accs.append(train_acc)  # 存储训练准确率\n\n        # 验证阶段\n        model.eval()  # 设置模型为评估模式\n        val_loss = 0\n        val_preds = []  # 用于存储验证集的预测结果\n        val_true = []  # 用于存储验证集的真实标签\n\n        with torch.no_grad():  # 不计算梯度\n            for input_ids, attention_mask, ratings in val_loader:\n                input_ids = input_ids.to(device)  # 将数据移动到GPU\n                attention_mask = attention_mask.to(device)  # 将数据移动到GPU\n                ratings = ratings.to(device)  # 将数据移动到GPU\n\n                outputs = model(input_ids, attention_mask)  # 获取模型输出\n                loss = criterion(outputs, ratings)  # 计算损失\n                val_loss += loss.item()  # 累加损失\n\n                # 收集预测结果\n                _, predicted = torch.max(outputs.data, 1)  # 获取预测类别\n                val_preds.extend(predicted.cpu().numpy())  # 存储预测结果\n                val_true.extend(ratings.cpu().numpy())  # 存储真实标签\n\n        val_loss /= len(val_loader)  # 计算平均验证损失\n        val_acc = accuracy_score(val_true, val_preds)  # 计算验证准确率\n        val_losses.append(val_loss)  # 存储验证损失\n        val_accs.append(val_acc)  # 存储验证准确率\n\n        # 打印训练和验证信息\n        print(f&quot;Epoch {epoch+1}/{epochs}&quot;)\n        print(f&quot;Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}&quot;)\n        print(f&quot;Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}&quot;)\n\n        # 每个epoch结束时打印详细的分类报告\n        if epoch % 5 == 0:  # 每5个epoch打印一次详细报告\n            print(&quot;\\nClassification Report:&quot;)\n            print(classification_report(val_true, val_preds,\n                                        target_names=['1 星', '2 星', '3 星', '4 星', '5 星']))  # 打印分类报告\n            plot_confusion_matrix(val_true, val_preds)  # 绘制混淆矩阵\n\n        print(&quot;-&quot; * 50)\n\n        # 学习率调整\n        scheduler.step(val_loss)  # 根据验证损失调整学习率\n\n        # 早停机制\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # 保存最佳模型\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),  # 保存模型参数\n                'optimizer_state_dict': optimizer.state_dict(),  # 保存优化器参数\n                'val_loss': val_loss,  # 保存最佳验证损失\n                'val_acc': val_acc  # 保存最佳验证准确率\n            }, 'best_model.pth')  # 保存模型到文件\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                print(&quot;Early stopping triggered&quot;)  # 触发早停\n                break\n\n    # 绘制训练曲线\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label=&quot;训练损失&quot;)\n    plt.plot(val_losses, label=&quot;验证损失&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;损失&quot;)\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label=&quot;训练准确率&quot;)\n    plt.plot(val_accs, label=&quot;验证准确率&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;准确率&quot;)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return train_losses, val_losses, train_accs, val_accs\n\n# 开始训练\ntrain_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n</code></pre>\n<p>主要改进：</p>\n<p>双向LSTM: 使用双向LSTM捕捉文本序列的双向信息。\n注意力机制: 引入注意力机制，使模型能够关注更重要的词语信息。\n更深的网络: 使用更多的LSTM层和全连接层，增加模型的复杂度和表达能力。\nDropout: 在多个层之间加入Dropout，防止过拟合。\n梯度裁剪: 使用梯度裁剪防止梯度爆炸，提高训练稳定性。\nAdamW优化器: 使用AdamW优化器，可以更好地处理权重衰减。\nReduceLROnPlateau学习率调度器: 当验证损失停止下降时降低学习率，帮助模型找到更好的最优解。\n更详细的评估: 使用classification_report和confusion_matrix对模型进行更详细的评估。\n模型结构可以概括为以下几个步骤：</p>\n<p>BERT词嵌入: 使用预训练的BERT模型对文本进行编码，将每个词转换为一个高维向量表示。</p>\n<p>双向LSTM: 将BERT词嵌入序列输入到双向LSTM中，提取更丰富的文本特征。</p>\n<p>注意力机制:  使用注意力机制对LSTM的输出进行加权求和，突出重要的词语信息。</p>\n<p>全连接层: 注意力机制的输出经过三层全连接层，将高维特征映射到五个类别（对应五种星级）。</p>\n<p>Softmax: 全连接层的输出经过Softmax激活函数，得到每个类别的预测概率。</p>\n<h2>模型2</h2>\n<pre><code class=\"language-python\"># 导入所需库\nfrom datasets import load_dataset  # 用于加载Hugging Face数据集\nimport matplotlib.pyplot as plt  # 用于绘图\nimport pandas as pd  # 用于数据处理\nfrom sklearn.model_selection import train_test_split  # 用于划分数据集\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset  # 用于创建数据加载器和数据集\nfrom transformers import BertTokenizer  # 用于加载BERT分词器\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # 用于评估模型性能\nimport seaborn as sns  # 用于绘制热力图\n\n# 检查CUDA是否可用\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f&quot;Using device: {device}&quot;)\n\n# 加载数据集，并取前20000条数据\ndataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\ndata = pd.DataFrame(dataset[&quot;full&quot;][:20000])\nprint(data.head())\n\n# 设置Matplotlib的字体参数\nplt.rcParams['font.family'] = 'SimHei'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 初始化BERT分词器\ntokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmax_len = 128  # 设置最大序列长度\n\n\n# ReviewDataset 类修改，合并标签\nclass ReviewDataset(Dataset):\n    def __init__(self, texts, ratings):\n        self.texts = texts\n        self.ratings = ratings\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # 使用BertTokenizer对文本进行编码\n        encoded = tokenizer.encode_plus(\n            self.texts[idx],\n            add_special_tokens=True,  # 添加特殊标记\n            max_length=max_len,  # 设置最大长度\n            padding=&quot;max_length&quot;,  # 使用最大长度进行填充\n            truncation=True,  # 超过最大长度进行截断\n            return_tensors=&quot;pt&quot;,  # 返回pytorch张量\n        )\n        input_ids = encoded[&quot;input_ids&quot;].squeeze()  # 获取input_ids\n        attention_mask = encoded[&quot;attention_mask&quot;].squeeze()  # 获取attention_mask\n\n        # 合并标签：将 5 个星级评分合并为 3 个类别\n        original_rating = int(self.ratings[idx])\n        if original_rating &lt;= 2:\n            rating = 0  # 1-2 星合并为 0 类\n        elif original_rating &lt;= 4:\n            rating = 1  # 3-4 星合并为 1 类\n        else:\n            rating = 2  # 5 星为 2 类\n\n        return input_ids, attention_mask, torch.tensor(rating, dtype=torch.long)  # 返回输入ID、注意力掩码和合并后的标签\n\n\n# 增强版LSTM分类模型\nclass EnhancedReviewClassifier(nn.Module):\n    def __init__(self, embedding_dim=256, hidden_dim=256, num_layers=2, dropout=0.3, bidirectional=True, num_classes=3):\n        super(EnhancedReviewClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dim)  # 词嵌入层\n        self.dropout = nn.Dropout(dropout)  # Dropout层\n\n        # 双向LSTM\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=num_layers,  # LSTM层数\n            batch_first=True,\n            bidirectional=bidirectional,  # 是否双向\n            dropout=dropout if num_layers &gt; 1 else 0  # LSTM层之间添加Dropout\n        )\n\n        # 注意力机制\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim),  # 全连接层\n            nn.Tanh(),  # Tanh激活函数\n            nn.Linear(hidden_dim, 1),  # 全连接层\n            nn.Softmax(dim=1)  # Softmax激活函数，计算注意力权重\n        )\n\n        # 全连接层\n        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim  # 根据LSTM是否双向确定输入维度\n        self.fc1 = nn.Linear(fc_input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)  # 输出改为3个类别\n\n        self.relu = nn.ReLU()  # ReLU激活函数\n        self.softmax = nn.Softmax(dim=1)  # 添加softmax层，用于输出概率分布\n\n    def forward(self, input_ids, attention_mask):\n        # 词嵌入\n        embedded = self.embedding(input_ids)\n        embedded = self.dropout(embedded)\n\n        # LSTM层\n        lstm_out, (hidden, cell) = self.lstm(embedded)\n\n        # 注意力机制\n        attention_weights = self.attention(lstm_out)  # 计算注意力权重\n        attention_output = torch.sum(attention_weights * lstm_out, dim=1)  # 对LSTM输出进行加权求和\n\n        # 全连接层\n        x = self.dropout(attention_output)\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)  # 不在这里应用softmax，因为CrossEntropyLoss已包含\n\n        return x\n\n# 数据预处理和加载\ntrain_texts, val_texts, train_ratings, val_ratings = train_test_split(\n    data[&quot;text&quot;].fillna(&quot;&quot;), data[&quot;rating&quot;], test_size=0.2, random_state=42\n)\n\ntrain_dataset = ReviewDataset(train_texts.tolist(), train_ratings.tolist())\nval_dataset = ReviewDataset(val_texts.tolist(), val_ratings.tolist())\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# 初始化模型、损失函数和优化器\nmodel = EnhancedReviewClassifier().to(device)\ncriterion = nn.CrossEntropyLoss()  # 改用CrossEntropyLoss，适用于多分类任务\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)  # 使用AdamW优化器\n# 使用ReduceLROnPlateau学习率调度器，当验证损失停止下降时降低学习率\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)  \n\n# 绘制混淆矩阵\ndef plot_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # 绘制热力图\n    plt.title('混淆矩阵')\n    plt.xlabel('预测类别')\n    plt.ylabel('真实类别')\n    plt.show()\n\n# 训练模型\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=20, patience=5):\n    train_losses, val_losses = [], []  # 用于存储训练和验证损失\n    train_accs, val_accs = [], []  # 用于存储训练和验证准确率\n    best_val_loss = float(&quot;inf&quot;)  # 初始化最佳验证损失\n    patience_counter = 0  # 初始化早停计数器\n\n    for epoch in range(epochs):\n        # 训练阶段\n        model.train()  # 设置模型为训练模式\n        running_loss = 0\n        train_preds = []  # 用于存储训练集的预测结果\n        train_true = []  # 用于存储训练集的真实标签\n\n        for input_ids, attention_mask, ratings in train_loader:\n            input_ids = input_ids.to(device)  # 将数据移动到GPU\n            attention_mask = attention_mask.to(device)  # 将数据移动到GPU\n            ratings = ratings.to(device)  # 将数据移动到GPU\n\n            optimizer.zero_grad()  # 清空梯度\n            outputs = model(input_ids, attention_mask)  # 获取模型输出\n            loss = criterion(outputs, ratings)  # 计算损失\n            loss.backward()  # 反向传播\n\n            # 梯度裁剪，防止梯度爆炸\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n\n            optimizer.step()  # 更新模型参数\n            running_loss += loss.item()  # 累加损失\n\n            # 收集预测结果\n            _, predicted = torch.max(outputs.data, 1)  # 获取预测类别\n            train_preds.extend(predicted.cpu().numpy())  # 存储预测结果\n            train_true.extend(ratings.cpu().numpy())  # 存储真实标签\n\n        train_loss = running_loss / len(train_loader)  # 计算平均训练损失\n        train_acc = accuracy_score(train_true, train_preds)  # 计算训练准确率\n        train_losses.append(train_loss)  # 存储训练损失\n        train_accs.append(train_acc)  # 存储训练准确率\n\n        # 验证阶段\n        model.eval()  # 设置模型为评估模式\n        val_loss = 0\n        val_preds = []  # 用于存储验证集的预测结果\n        val_true = []  # 用于存储验证集的真实标签\n\n        with torch.no_grad():  # 不计算梯度\n            for input_ids, attention_mask, ratings in val_loader:\n                input_ids = input_ids.to(device)  # 将数据移动到GPU\n                attention_mask = attention_mask.to(device)  # 将数据移动到GPU\n                ratings = ratings.to(device)  # 将数据移动到GPU\n\n                outputs = model(input_ids, attention_mask)  # 获取模型输出\n                loss = criterion(outputs, ratings)  # 计算损失\n                val_loss += loss.item()  # 累加损失\n\n                # 收集预测结果\n                _, predicted = torch.max(outputs.data, 1)  # 获取预测类别\n                val_preds.extend(predicted.cpu().numpy())  # 存储预测结果\n                val_true.extend(ratings.cpu().numpy())  # 存储真实标签\n\n        val_loss /= len(val_loader)  # 计算平均验证损失\n        val_acc = accuracy_score(val_true, val_preds)  # 计算验证准确率\n        val_losses.append(val_loss)  # 存储验证损失\n        val_accs.append(val_acc)  # 存储验证准确率\n\n        # 打印训练和验证信息\n        print(f&quot;Epoch {epoch+1}/{epochs}&quot;)\n        print(f&quot;Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}&quot;)\n        print(f&quot;Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}&quot;)\n\n        # 每个epoch结束时打印详细的分类报告\n        if epoch % 5 == 0:  # 每5个epoch打印一次详细报告\n            print(&quot;\\nClassification Report:&quot;)\n            # 打印分类报告，注意目标类别名称已更改\n            print(classification_report(val_true, val_preds, \n                                        target_names=['1 ~2星', '3 ~4星', '5 星']))  \n            plot_confusion_matrix(val_true, val_preds)  # 绘制混淆矩阵\n\n        print(&quot;-&quot; * 50)\n\n        # 学习率调整\n        scheduler.step(val_loss)  # 根据验证损失调整学习率\n\n        # 早停机制\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # 保存最佳模型\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),  # 保存模型参数\n                'optimizer_state_dict': optimizer.state_dict(),  # 保存优化器参数\n                'val_loss': val_loss,  # 保存最佳验证损失\n                'val_acc': val_acc  # 保存最佳验证准确率\n            }, 'best_model.pth')  # 保存模型到文件\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                print(&quot;Early stopping triggered&quot;)  # 触发早停\n                break\n\n    # 绘制训练曲线\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label=&quot;训练损失&quot;)\n    plt.plot(val_losses, label=&quot;验证损失&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;损失&quot;)\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label=&quot;训练准确率&quot;)\n    plt.plot(val_accs, label=&quot;验证准确率&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;准确率&quot;)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return train_losses, val_losses, train_accs, val_accs\n\n# 开始训练\ntrain_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n</code></pre>\n<p>模型结构可以概括为以下几个步骤：</p>\n<p>BERT词嵌入: 使用预训练的BERT模型对文本进行编码，将每个词转换为一个高维向量表示。</p>\n<p>双向LSTM: 将BERT词嵌入序列输入到双向LSTM中，提取更丰富的文本特征。</p>\n<p>注意力机制:  使用注意力机制对LSTM的输出进行加权求和，突出重要的词语信息。</p>\n<p>全连接层: 注意力机制的输出经过三层全连接层，将高维特征映射到三个类别。</p>\n<p>Softmax: 全连接层的输出经过Softmax激活函数，得到每个类别的预测概率。</p>\n<p>合并标签的主要目的：</p>\n<p>减少类别数量: 简化分类任务，使模型更容易学习。\n解决类别不平衡: 原始数据集中，5 星评论的数量可能远多于其他星级，合并标签可以缓解类别不平衡问题。</p>\n"}},277:function(n){n.exports={attributes:{title:"亚马逊评论数据做文本情感分类模型补充",date:"2024-11-18T00:00:00.000Z",summary:"在尝试的学习NLP技术",coverImage:"11-19-9.jpg",pinned:!0},html:"<h1>在bert上做微调模型</h1>\n<p>模型改动太多了，干脆直接发个最终模型得了。</p>\n<pre><code class=\"language-python\"># 导入所需库\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n\n# 检查CUDA是否可用\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f&quot;Using device: {device}&quot;)\n\n# 加载数据集，并取前20000条数据\ndataset = load_dataset(&quot;McAuley-Lab/Amazon-Reviews-2023&quot;, &quot;raw_review_All_Beauty&quot;, trust_remote_code=True)\ndata = pd.DataFrame(dataset[&quot;full&quot;][:200000])\nprint(data.head())\n\n# 设置Matplotlib的字体参数\nplt.rcParams['font.family'] = 'SimHei'\nplt.rcParams['axes.unicode_minus'] = False\n\n# 使用BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)\nmax_len = 128\n\nclass ReviewDataset(Dataset):\n    def __init__(self, texts, ratings):\n        self.texts = texts\n        self.ratings = ratings\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encoded = tokenizer.encode_plus(\n            self.texts[idx],\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoded['input_ids'].squeeze()\n        attention_mask = encoded['attention_mask'].squeeze()\n        \n        # 合并标签\n        original_rating = int(self.ratings[idx])\n        if original_rating &lt;= 2:\n            rating = 0  # 1-2 星合并为 0 类\n        elif original_rating &lt;= 4:\n            rating = 1  # 3-4 星合并为 1 类\n        else:\n            rating = 2  # 5 星为 2 类\n        return input_ids, attention_mask, torch.tensor(rating, dtype=torch.long)\n\n# BERT分类器模型\nclass BERTReviewClassifier(nn.Module):\n    def __init__(self, dropout=0.3, num_classes=3):\n        super(BERTReviewClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        \n        # BERT的隐藏层大小\n        hidden_size = self.bert.config.hidden_size\n        \n        # 分类头\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_size // 2, num_classes)\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        # 获取BERT输出\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # 使用[CLS]标记的输出进行分类\n        pooled_output = outputs.pooler_output\n        \n        # 通过分类头\n        logits = self.classifier(pooled_output)\n        \n        return logits\n\n# 数据预处理和加载\ntrain_texts, val_texts, train_ratings, val_ratings = train_test_split(\n    data[&quot;text&quot;].fillna(&quot;&quot;), data[&quot;rating&quot;], test_size=0.2, random_state=42\n)\n\ntrain_dataset = ReviewDataset(train_texts.tolist(), train_ratings.tolist())\nval_dataset = ReviewDataset(val_texts.tolist(), val_ratings.tolist())\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # 减小batch size\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# 初始化模型、损失函数和优化器\nmodel = BERTReviewClassifier().to(device)\ncriterion = nn.CrossEntropyLoss()\n\n# 设置不同的学习率\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay': 0.01},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay': 0.0}\n]\noptimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=2e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\ndef plot_confusion_matrix(y_true, y_pred,epoch):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.title('混淆矩阵')\n    plt.xlabel('预测类别')\n    plt.ylabel('真实类别')\n    import os\n    # 确保保存目录存在\n    if not os.path.exists(&quot;figures&quot;):\n        os.makedirs(&quot;figures&quot;)\n\n    # 保存图像而不是显示\n    plt.savefig(&quot;figures/epoch%s-matrix.png&quot;%epoch)\n    plt.close()\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, patience=3):\n    train_losses, val_losses = [], []\n    train_accs, val_accs = [], []\n    best_val_loss = float(&quot;inf&quot;)\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        # 训练阶段\n        model.train()\n        running_loss = 0\n        train_preds = []\n        train_true = []\n        \n        for input_ids, attention_mask, ratings in train_loader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            ratings = ratings.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, ratings)\n            loss.backward()\n            \n            # 梯度裁剪\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            running_loss += loss.item()\n            \n            # 收集预测结果\n            _, predicted = torch.max(outputs.data, 1)\n            train_preds.extend(predicted.cpu().numpy())\n            train_true.extend(ratings.cpu().numpy())\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = accuracy_score(train_true, train_preds)\n        train_losses.append(train_loss)\n        train_accs.append(train_acc)\n\n        # 验证阶段\n        model.eval()\n        val_loss = 0\n        val_preds = []\n        val_true = []\n        \n        with torch.no_grad():\n            for input_ids, attention_mask, ratings in val_loader:\n                input_ids = input_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                ratings = ratings.to(device)\n                \n                outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs, ratings)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(outputs.data, 1)\n                val_preds.extend(predicted.cpu().numpy())\n                val_true.extend(ratings.cpu().numpy())\n        \n        val_loss /= len(val_loader)\n        val_acc = accuracy_score(val_true, val_preds)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n        \n        print(f&quot;Epoch {epoch+1}/{epochs}&quot;)\n        print(f&quot;Training Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}&quot;)\n        print(f&quot;Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}&quot;)\n        \n        # 每个epoch结束时打印详细的分类报告\n        if epoch % 2 == 0:  # 每2个epoch打印一次详细报告\n            print(&quot;\\nClassification Report:&quot;)\n            print(classification_report(val_true, val_preds, \n                                     target_names=['1-2星', '3-4星', '5星']))\n            plot_confusion_matrix(val_true, val_preds,epoch)\n            \n        print(&quot;-&quot; * 50)\n\n        # 学习率调整\n        scheduler.step(val_loss)\n\n        # 早停机制\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # 保存最佳模型\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_loss,\n                'val_acc': val_acc\n            }, 'best_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                print(&quot;Early stopping triggered&quot;)\n                break\n\n    # 绘制训练曲线\n    plt.figure(figsize=(12, 4))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(train_losses, label=&quot;训练损失&quot;)\n    plt.plot(val_losses, label=&quot;验证损失&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;损失&quot;)\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(train_accs, label=&quot;训练准确率&quot;)\n    plt.plot(val_accs, label=&quot;验证准确率&quot;)\n    plt.xlabel(&quot;轮次&quot;)\n    plt.ylabel(&quot;准确率&quot;)\n    plt.legend()\n    \n    plt.tight_layout()\n    import os\n    # 确保保存目录存在\n    if not os.path.exists(&quot;figures&quot;):\n        os.makedirs(&quot;figures&quot;)\n\n    # 保存图像而不是显示\n    plt.savefig(&quot;figures/loss_curve.png&quot;)\n    plt.close()\n\n    return train_losses, val_losses, train_accs, val_accs\n\n# 开始训练\ntrain_losses, val_losses, train_accs, val_accs = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n</code></pre>\n<pre><code class=\"language-python\">\n</code></pre>\n<pre><code></code></pre>\n"}},8774:function(n){n.exports={attributes:{title:"记录一下写过的爬虫脚本",date:"2024-11-16T00:00:00.000Z",summary:"写的都是js脚本",coverImage:"11-19-8.jpg",pinned:!1},html:"<p><s>不会有人还在写python脚本吧？</s>  <br>\n无论是什么语言写的脚本，都必须要操作DOM  <br>\n既然都操作都操作DOM了，我何不用JS来写呢？ \\</p>\n<p>下文记录一下自己写过的爬虫脚本，数据整理代码写在另一篇了。</p>\n<h1>pipiads的爬虫脚本</h1>\n<p>pipiads上面主要有tiktok小店的数据，和一些广告的数据。</p>\n<h2>主要是爬取商品数据的信息</h2>\n<pre><code class=\"language-javascript\">\n// ==UserScript==\n// @name         皮皮一键爬取\n// @namespace    http://tampermonkey.net/\n// @version      2024-09-14\n// @description  try to take over the world!\n// @author       suxing\n// @match        https://www.pipiads.com/*\n// @icon         https://www.google.com/s2/favicons?sz=64&amp;domain=pipiads.com\n// @grant        none\n// ==/UserScript==\n\n\nfunction main(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;pipiads-button1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },4000)\n    }).then(()=&gt;{\n        let pageMain = document.querySelector(&quot;.main-container&quot;);\n        console.log(pageMain)\n        if(pageMain){\n            const task1Button = document.createElement(&quot;button&quot;);\n            task1Button.className = &quot;button-test&quot;;\n            task1Button.innerHTML = &quot;任务1 小店爬取&quot;;\n            styleButton(task1Button);\n            task1Button.onclick=()=&gt;{task1();}\n\n            const task2Button = document.createElement(&quot;button&quot;);\n            task2Button.className = &quot;button-test&quot;;\n            task2Button.innerHTML = &quot;任务2 产品爬取&quot;;\n            styleButton(task2Button);\n            task2Button.onclick=()=&gt;{task2();}\n            \n\n            var buttonContainer = document.createElement(&quot;div&quot;);\n            buttonContainer.style.display = &quot;flex&quot;;\n            buttonContainer.style.justifyContent = &quot;center&quot;;\n            buttonContainer.style.alignItems = &quot;center&quot;;\n            buttonContainer.style.height = &quot;100px&quot;;\n            buttonContainer.appendChild(task1Button);\n            buttonContainer.appendChild(task2Button);\n            pageMain.parentNode.insertBefore(buttonContainer, pageMain);\n\n\n        }\n    })\n}\n\nfunction exportjson(data,taskname){\n    const blob = new Blob([data], { type: 'application/json' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    const today = new Date();\n    const year = today.getFullYear();\n    const month = String(today.getMonth() + 1).padStart(2, '0');\n    const day = String(today.getDate()).padStart(2, '0');\n    const formattedDate = `${year}-${month}-${day}`;\n    a.download = `${taskname}-${formattedDate}.json`;\n    document.body.appendChild(a);\n    a.click();\n    document.body.removeChild(a);\n    URL.revokeObjectURL(url);\n}\nfunction task2(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;pipiads-scraper test1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },700)\n    }).then(()=&gt;{\n        new Promise((resolve)=&gt;{\n            window.scrollTo({\n                top: 7000,\n                behavior: &quot;smooth&quot;\n            });\n            setTimeout(()=&gt;{\n                resolve();\n            },1000)\n        }).then(()=&gt;{\n            new Promise((resolve)=&gt;{\n                window.scrollTo({\n                    top: 14000,\n                    behavior: &quot;smooth&quot;\n                });\n                setTimeout(()=&gt;{\n                    resolve();\n                },1000)\n            }).then(()=&gt;{\n                new Promise((resolve)=&gt;{\n                    window.scrollTo({\n                        top: 21000,\n                        behavior: &quot;smooth&quot;\n                    });\n                    setTimeout(()=&gt;{\n                        resolve();\n                    },1000)\n                }).then(()=&gt;{\n                    new Promise((resolve)=&gt;{\n                        window.scrollTo({\n                            top: document.body.scrollHeight,\n                            behavior: &quot;smooth&quot;\n                        });\n                        setTimeout(()=&gt;{\n                            resolve();\n                        },700)\n                    }).then(()=&gt;{\n                        let productlist=[];\n                        document.querySelectorAll(&quot;ul.wt-block-grid&gt;li.wt-block-grid__item&quot;).forEach(li=&gt;{\n                            let title=li.querySelector(&quot;a.title.a-link&quot;).innerText;\n                            let url=li.querySelector(&quot;a.title.a-link&quot;).getAttribute(&quot;href&quot;);\n                            let price=&quot;&quot;\n                            try{\n                                price=li.querySelector(&quot;.price-box&quot;).innerText.replace(/[()]/g, &quot;&quot;);\n                            }catch{\n                                price=&quot;&quot;;\n                            }\n                            let datablock=li.querySelector(&quot;.data-count&quot;);\n                            var i=0;\n                            let temp=[];\n                            datablock.querySelectorAll(&quot;.item&quot;).forEach(item=&gt;{\n                                if(i==0){let thumb=item.querySelector(&quot;.value&quot;).innerText;temp.push(thumb)}\n                                if(i==1){let ads=item.querySelector(&quot;.value&quot;).innerText;temp.push(ads)}\n                                if(i==2){let increase=item.querySelector(&quot;.value&quot;).innerText;temp.push(increase)}\n                                i++;\n                            })\n                            let products={\n                                &quot;title&quot;:title,\n                                &quot;url&quot;:url,\n                                &quot;price&quot;:price,\n                                &quot;thumb&quot;:temp[0],\n                                &quot;ads&quot;:temp[1],\n                                &quot;increase&quot;:temp[2],\n                            }\n                            productlist.push(products);\n\n                        })\n                        const productListJson = JSON.stringify(productlist, null, 2);\n                        console.log(productListJson);\n                        exportjson(productListJson,&quot;task2&quot;);\n                        window.scrollTo({\n                            top: 0,\n                            behavior: &quot;smooth&quot;\n                        });\n                    })\n                })\n            })\n        })\n    })\n}\n\nfunction task1(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;pipiads-scraper test1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },700)\n    }).then(()=&gt;{\n        new Promise((resolve)=&gt;{\n            window.scrollTo({\n                top: 7000,\n                behavior: &quot;smooth&quot;\n            });\n            setTimeout(()=&gt;{\n                resolve();\n            },1000)\n        }).then(()=&gt;{\n            new Promise((resolve)=&gt;{\n                window.scrollTo({\n                    top: 14000,\n                    behavior: &quot;smooth&quot;\n                });\n                setTimeout(()=&gt;{\n                    resolve();\n                },1000)\n            }).then(()=&gt;{\n                new Promise((resolve)=&gt;{\n                    window.scrollTo({\n                        top: 21000,\n                        behavior: &quot;smooth&quot;\n                    });\n                    setTimeout(()=&gt;{\n                        resolve();\n                    },1000)\n                }).then(()=&gt;{\n                    new Promise((resolve)=&gt;{\n                        window.scrollTo({\n                            top: document.body.scrollHeight,\n                            behavior: &quot;smooth&quot;\n                        });\n                        setTimeout(()=&gt;{\n                            resolve();\n                        },700)\n                    }).then(()=&gt;{\n                        let productlist=[];\n                        document.querySelectorAll(&quot;ul.wt-block-grid&gt;li.wt-block-grid__item&quot;).forEach(li=&gt;{\n                            let title=li.querySelector(&quot;a.title.a-link&quot;).innerText;\n                            let url=li.querySelector(&quot;a.title.a-link&quot;).getAttribute(&quot;href&quot;);\n                            let price=&quot;&quot;\n                            try{\n                                price=li.querySelector(&quot;span.usdPrice&quot;).innerText.replace(/[()]/g, &quot;&quot;);\n                            }catch{\n                                price=li.querySelector(&quot;strong.price&quot;).innerText;\n                            }\n                            let quantity=li.querySelector(&quot;.sales-value&quot;).innerText;\n                            let datablock=li.querySelector(&quot;.data-count&quot;);\n                            let adsblock=li.querySelector(&quot;.time-data-box&quot;);\n                            var i=0;\n                            let temp=[];\n                            datablock.querySelectorAll(&quot;.item&quot;).forEach(item=&gt;{\n                                if(i==0){let ads=item.querySelector(&quot;.value&quot;).innerText;temp.push(ads)}\n                                if(i==1){let thumb=item.querySelector(&quot;.value&quot;).innerText;temp.push(thumb)}\n                                if(i==2){let thumbrate=item.querySelector(&quot;.value&quot;).innerText;temp.push(thumbrate)}\n                                i++;\n                            })\n                            let temp1=[]\n                            var i1=0\n                            adsblock.querySelectorAll(&quot;.time-item&quot;).forEach(item=&gt;{\n                                if(i1==0){let starttime=item.querySelector(&quot;._value&quot;).innerText;temp1.push(starttime)}\n                                if(i1==1){let endtime=item.querySelector(&quot;._value&quot;).innerText;temp1.push(endtime)}\n                                if(i1==2){let nums=item.querySelector(&quot;._value&quot;).innerText;temp1.push(nums)}\n                                i1++;\n\n                            })\n                            let products={\n                                &quot;title&quot;:title,\n                                &quot;url&quot;:url,\n                                &quot;price&quot;:price,\n                                &quot;quantity&quot;:quantity,\n                                &quot;ads&quot;:temp[0],\n                                &quot;thumb&quot;:temp[1],\n                                &quot;thumbrate&quot;:temp[2],\n                                &quot;starttime&quot;:temp1[0],\n                                &quot;endtime&quot;:temp1[1],\n                                &quot;adsnums&quot;:temp1[2]\n                            }\n                            productlist.push(products);\n\n                        })\n                        const productListJson = JSON.stringify(productlist, null, 2);\n                        console.log(productListJson);\n                        exportjson(productListJson,&quot;task1&quot;);\n                        window.scrollTo({\n                            top: 0,\n                            behavior: &quot;smooth&quot;\n                        });\n                    })\n                })\n            })\n        })\n    })\n}\nfunction styleButton(button) {\n    button.style.padding = &quot;7px 15px&quot;;\n    button.style.margin = &quot;5px&quot;;\n    button.style.fontSize = &quot;16px&quot;;\n    button.style.backgroundColor = &quot;#4CAF50&quot;;\n    button.style.color = &quot;white&quot;;\n    button.style.border = &quot;none&quot;;\n    button.style.borderRadius = &quot;5px&quot;;\n    button.style.cursor = &quot;pointer&quot;;\n    button.style.boxShadow = &quot;0px 4px 6px rgba(0, 0, 0, 0.1)&quot;;\n    button.style.transition = &quot;background-color 0.3s&quot;;\n    button.onmouseover = function() {\n        button.style.backgroundColor = &quot;#45a049&quot;;\n    };\n    button.onmouseout = function() {\n        button.style.backgroundColor = &quot;#4CAF50&quot;;\n    };\n}\n\n\n\n\nwindow.addEventListener(&quot;load&quot;,()=&gt;{\n    main();\n},false)\n</code></pre>\n<h1>sellercenter爬虫</h1>\n<p>当时随便找到的网站，上面不少数据是可以免费查看的，很爽。\n随手就搞了一个爬虫，虽然没怎么用。</p>\n<pre><code class=\"language-javascript\">// ==UserScript==\n// @name         ebayscraper1\n// @namespace    http://tampermonkey.net/\n// @version      2024-10-14\n// @description  try to take over the world!\n// @author       suxing\n// @match        https://sellercenter.io/*\n// @icon         https://www.google.com/s2/favicons?sz=64&amp;domain=ebay.com\n// @grant        none\n// ==/UserScript==\n\n\nfunction main(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;sc-test!&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },2000)\n    }).then(()=&gt;{\n        let pageMain = document.querySelector(&quot;.el-table__header-wrapper&quot;);\n        console.log(pageMain)\n        if(pageMain){\n            let button = document.createElement(&quot;button&quot;);\n            button.className=&quot;button-test&quot;\n            button.innerHTML = &quot;点击爬取json&quot;;\n            button.style.padding = &quot;10px 20px&quot;;\n            button.style.fontSize = &quot;16px&quot;;\n            button.style.backgroundColor = &quot;#4CAF50&quot;;\n            button.style.color = &quot;white&quot;;\n            button.style.border = &quot;none&quot;;\n            button.style.borderRadius = &quot;5px&quot;;\n            button.style.cursor = &quot;pointer&quot;;\n            button.style.boxShadow = &quot;0px 4px 6px rgba(0, 0, 0, 0.1)&quot;;\n            button.style.transition = &quot;background-color 0.3s&quot;;\n            button.onmouseover = function() {\n                button.style.backgroundColor = &quot;#45a049&quot;;\n            };\n            button.onmouseout = function() {\n                button.style.backgroundColor = &quot;#4CAF50&quot;;\n            };\n            button.onclick=()=&gt;{scrapefunc();}\n            var buttonContainer = document.createElement(&quot;div&quot;);\n            buttonContainer.style.display = &quot;flex&quot;;\n            buttonContainer.style.justifyContent = &quot;center&quot;;\n            buttonContainer.style.alignItems = &quot;center&quot;;\n            buttonContainer.style.height = &quot;100px&quot;; // 调整高度以便更好地居中\n            buttonContainer.appendChild(button);\n\n            // 在 pageMain 元素的上方插入按钮\n            pageMain.parentNode.insertBefore(buttonContainer, pageMain);\n        }\n    })\n}\n\nfunction exportjson(data){\n    const blob = new Blob([data], { type: 'application/json' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    const today = new Date();\n    const year = today.getFullYear();\n    const month = String(today.getMonth() + 1).padStart(2, '0');  \n    const day = String(today.getDate()).padStart(2, '0');  \n    const formattedDate = `${year}-${month}-${day}`;\n    a.download = `ebay${formattedDate}.json`;\n    document.body.appendChild(a);\n    a.click();\n    document.body.removeChild(a);\n    URL.revokeObjectURL(url);\n}\nfunction scrapefunc(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;ebay-scraper test1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },1000)\n    }).then(()=&gt;{\n        new Promise((resolve)=&gt;{\n            window.scrollTo({\n                top: 2000,\n                behavior: &quot;smooth&quot;\n            });\n            setTimeout(()=&gt;{\n                resolve();\n            },1000)\n        }).then(()=&gt;{\n            let productlist=[];\n            document.querySelectorAll(&quot;.el-table__row &quot;).forEach(li=&gt;{\n                let title=li.querySelector(&quot;.productNameNew&quot;).innerText;\n                \n                let price=li.querySelector(&quot;.priceShow&quot;).innerText  \n                let cate=li.querySelector(&quot;.el-tooltip__trigger&quot;).innerText\n                let quantity=&quot;&quot;   \n                try{\n                    quantity=li.querySelector(&quot;.cell&gt;span&quot;).innerText;\n                }catch{\n                    quantity=&quot;不显示&quot;\n                }                   \n                let info =&quot;&quot;\n                try{\n                    info=li.querySelector(&quot;.viewLabel&quot;).innerText; \n        \n                }\n                catch{\n                    shipping=&quot;不显示&quot;\n                }\n\n                \n                let products={\n                    &quot;title&quot;:title,\n                    &quot;cate&quot;:cate,\n                    &quot;price&quot;:price,\n                    &quot;quantity&quot;:quantity,\n                    &quot;info&quot;:info,\n\n                }                                                                      \n                productlist.push(products);\n                })\n                const productListJson = JSON.stringify(productlist, null, 2);\n                console.log(productListJson);\n                exportjson(productListJson);\n                window.scrollTo({\n                    top: 0,\n                    behavior: &quot;smooth&quot;\n                });\n            })        \n\n        })\n}\n\n\n\n\n\nwindow.addEventListener(&quot;load&quot;,()=&gt;{ \n    main();\n\n},false)\n</code></pre>\n<h1>ebay爬虫</h1>\n<p>当时做客户需求搞的脚本，爬了二手服务器，内存条，硬盘数据</p>\n<pre><code class=\"language-javascipt\">// ==UserScript==\n// @name         ebayscraper1\n// @namespace    http://tampermonkey.net/\n// @version      2024-10-14\n// @description  try to take over the world!\n// @author       suxing\n// @match        https://www.ebay.com/*\n// @icon         https://www.google.com/s2/favicons?sz=64&amp;domain=ebay.com\n// @grant        none\n// ==/UserScript==\n\n\nfunction main(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;pipiads-button1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },2000)\n    }).then(()=&gt;{\n        let pageMain = document.querySelector(&quot;.s-answer-region&quot;);\n        console.log(pageMain)\n        if(pageMain){\n            let button = document.createElement(&quot;button&quot;);\n            button.className=&quot;button-test&quot;\n            button.innerHTML = &quot;点击爬取json&quot;;\n            button.style.padding = &quot;10px 20px&quot;;\n            button.style.fontSize = &quot;16px&quot;;\n            button.style.backgroundColor = &quot;#4CAF50&quot;;\n            button.style.color = &quot;white&quot;;\n            button.style.border = &quot;none&quot;;\n            button.style.borderRadius = &quot;5px&quot;;\n            button.style.cursor = &quot;pointer&quot;;\n            button.style.boxShadow = &quot;0px 4px 6px rgba(0, 0, 0, 0.1)&quot;;\n            button.style.transition = &quot;background-color 0.3s&quot;;\n            button.onmouseover = function() {\n                button.style.backgroundColor = &quot;#45a049&quot;;\n            };\n            button.onmouseout = function() {\n                button.style.backgroundColor = &quot;#4CAF50&quot;;\n            };\n            button.onclick=()=&gt;{scrapefunc();}\n            var buttonContainer = document.createElement(&quot;div&quot;);\n            buttonContainer.style.display = &quot;flex&quot;;\n            buttonContainer.style.justifyContent = &quot;center&quot;;\n            buttonContainer.style.alignItems = &quot;center&quot;;\n            buttonContainer.style.height = &quot;100px&quot;; // 调整高度以便更好地居中\n            buttonContainer.appendChild(button);\n\n            // 在 pageMain 元素的上方插入按钮\n            pageMain.parentNode.insertBefore(buttonContainer, pageMain);\n        }\n    })\n}\n\nfunction exportjson(data){\n    const blob = new Blob([data], { type: 'application/json' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    const today = new Date();\n    const year = today.getFullYear();\n    const month = String(today.getMonth() + 1).padStart(2, '0');  \n    const day = String(today.getDate()).padStart(2, '0');  \n    const formattedDate = `${year}-${month}-${day}`;\n    a.download = `ebay${formattedDate}.json`;\n    document.body.appendChild(a);\n    a.click();\n    document.body.removeChild(a);\n    URL.revokeObjectURL(url);\n}\nfunction scrapefunc(){\n    new Promise((resolve)=&gt;{\n        console.log(&quot;ebay-scraper test1&quot;);\n        setTimeout(()=&gt;{\n            resolve();\n        },1000)\n    }).then(()=&gt;{\n        new Promise((resolve)=&gt;{\n            window.scrollTo({\n                top: 2000,\n                behavior: &quot;smooth&quot;\n            });\n            setTimeout(()=&gt;{\n                resolve();\n            },1000)\n        }).then(()=&gt;{\n            new Promise((resolve)=&gt;{\n                window.scrollTo({\n                    top: 5000,\n                    behavior: &quot;smooth&quot;\n                });\n                setTimeout(()=&gt;{\n                    resolve();\n                },1000)\n            }).then(()=&gt;{\n                new Promise((resolve)=&gt;{\n                    window.scrollTo({\n                        top: 8000,\n                        behavior: &quot;smooth&quot;\n                    });\n                    setTimeout(()=&gt;{\n                        resolve();\n                    },1000)\n                }).then(()=&gt;{\n                    new Promise((resolve)=&gt;{\n                        window.scrollTo({\n                            top: 12000,\n                            behavior: &quot;smooth&quot;\n                        });\n                        setTimeout(()=&gt;{\n                            resolve();\n                        },700)\n                    }).then(()=&gt;{    \n                        new Promise((resolve)=&gt;{\n                            window.scrollTo({\n                                top: 14000,\n                                behavior: &quot;smooth&quot;\n                            });\n                            setTimeout(()=&gt;{\n                                resolve();\n                            },700)\n    \n                        }).then(()=&gt;{\n                            let productlist=[];\n                            document.querySelectorAll(&quot;.s-item__info &quot;).forEach(li=&gt;{\n                                let title=li.querySelector(&quot;.s-item__title&gt;span&quot;).innerText;\n                                let secondtitle=&quot;&quot;\n                                try{\n                                    secondtitle=li.querySelector(&quot;.s-item__subtitle&gt;span&quot;).innerText;\n\n                                }\n                                catch{\n                                    secondtitle=&quot;不显示&quot;\n\n                                }\n                                \n                                let price=li.querySelector(&quot;span.s-item__price&quot;).innerText    \n                                let quantity=&quot;&quot;   \n                                try{\n                                    quantity=li.querySelector(&quot;span.s-item__quantitySold&quot;).innerText;\n                                }catch{\n                                    quantity=&quot;不显示&quot;\n                                }                   \n                                let loc=&quot;&quot;\n                                try{\n                                    loc=li.querySelector(&quot;span.s-item__location&quot;).innerText;\n\n                                }\n                                catch{\n                                    loc=&quot;不显示&quot;\n                                }\n                                let shipping =&quot;&quot;\n                                try{\n                                    shipping=li.querySelector(&quot;span.s-item__shipping&quot;).innerText; \n                      \n                                }\n                                catch{\n                                    shipping=&quot;不显示&quot;\n                                }\n    \n                                \n                                let products={\n                                    &quot;title&quot;:title,\n                                    &quot;secondtitle&quot;:secondtitle,\n                                    &quot;location&quot;:loc,\n                                    &quot;price&quot;:price,\n                                    &quot;quantity&quot;:quantity,\n                                    &quot;shipping&quot;:shipping,\n    \n                                }\n                                if (products.title!=&quot;Shop on eBay&quot;){\n                                    productlist.push(products);\n\n                                }\n                                \n                \n                            })\n                            const productListJson = JSON.stringify(productlist, null, 2);\n                            console.log(productListJson);\n                            exportjson(productListJson);\n                            let nextpage=document.querySelector(&quot;.pagination__next&quot;).href\n                            window.scrollTo({\n                                top: 0,\n                                behavior: &quot;smooth&quot;\n                            });\n                            location.href=nextpage\n                        })                              \n    \n                           \n                        \n                    })\n                })        \n\n            })\n               \n        })\n    })\n}\n\n\n\n\n\nwindow.addEventListener(&quot;load&quot;,()=&gt;{ \n    main();\n\n},false)\n</code></pre>\n<h1>tiktok爬虫</h1>\n<p>这个没什么卵用的脚本，做到一半我差不多就弃掉了。</p>\n<p>主要出发点就是，自己手点太慢了想搞个脚本一键保存tiktok后台数据不多好。</p>\n<p>结果发现还不如手点。。。所以全自动脚本等于全自己手动脚本。</p>\n<pre><code class=\"language-javascript\">// ==UserScript==\n// @name         tkshop test\n// @namespace    http://tampermonkey.net/\n// @version      2024-10-28\n// @description  字节跳动被我踩在脚下。\n// @author       suxing\n// @match        https://seller.tiktokglobalshop.com/*\n// @icon         https://www.google.com/s2/favicons?sz=64&amp;domain=tiktokglobalshop.com\n// @grant        none\n// ==/UserScript==\n\nfunction simulateClick(selector) {\n    const element = document.querySelector(selector);\n    \n    if (element) {\n        const rect = element.getBoundingClientRect();\n        const centerX = rect.left + rect.width / 2;\n        const centerY = rect.top + rect.height / 2;\n\n        // 创建鼠标移动事件\n        const mouseMoveEvent = new MouseEvent('mousemove', {\n            bubbles: true,\n            cancelable: true,\n            view: window,\n            clientX: centerX,\n            clientY: centerY\n        });\n\n        // 创建点击事件\n        const mouseDownEvent = new MouseEvent('mousedown', {\n            bubbles: true,\n            cancelable: true,\n            view: window,\n            clientX: centerX,\n            clientY: centerY\n        });\n\n        const mouseUpEvent = new MouseEvent('mouseup', {\n            bubbles: true,\n            cancelable: true,\n            view: window,\n            clientX: centerX,\n            clientY: centerY\n        });\n\n        const clickEvent = new MouseEvent('click', {\n            bubbles: true,\n            cancelable: true,\n            view: window,\n            clientX: centerX,\n            clientY: centerY\n        });\n\n        // 模拟鼠标移动\n        document.dispatchEvent(mouseMoveEvent);\n        // 模拟鼠标按下和松开\n        element.dispatchEvent(mouseDownEvent);\n        element.dispatchEvent(mouseUpEvent);\n        // 模拟点击\n        element.dispatchEvent(clickEvent);\n    } else {\n        console.error('Element not found for selector:', selector);\n    }\n}\n\nfunction delay(ms) {\n    return new Promise(resolve =&gt; setTimeout(resolve, ms));\n}\n\nasync function waitForModalToClose() {\n    // 等待打开的框消失\n    while (document.querySelector('div[role=&quot;dialog&quot;]')) {\n        await delay(500); // 每500毫秒检查一次\n    }\n}\n\nfunction main() {\n    new Promise((resolve) =&gt; {\n        console.log(&quot;tktest&quot;);\n        setTimeout(() =&gt; {\n            resolve();\n        }, 10000); // 等待 10 秒\n    }).then(() =&gt; {\n        const pageMain = document.querySelector(&quot;#compass-header&quot;);\n        console.log(pageMain);\n        if (pageMain) {\n            // 创建第一个按钮，用于第一个任务\n            const task1Button = document.createElement(&quot;button&quot;);\n            task1Button.className = &quot;button-test&quot;;\n            task1Button.innerHTML = &quot;任务 1 总览数据导出）&quot;;\n            styleButton(task1Button);\n            task1Button.onclick = async () =&gt; {\n                task1Button.disabled = true; // 禁用按钮以防重复点击\n                console.log(&quot;任务 1: 点击按钮&quot;);\n                // 执行任务 1 的操作\n                await simulateClick('button[data-tid=&quot;m4b_button&quot;]'); // 假设是导出按钮\n                task1Button.disabled = false; // 点击完成后启用按钮\n            };\n\n            // 创建第二个按钮，用于第二个任务\n            const task2Button = document.createElement(&quot;button&quot;);\n            task2Button.className = &quot;button-test&quot;;\n            task2Button.innerHTML = &quot;任务 2: 商品数据导出）&quot;;\n            styleButton(task2Button);\n            task2Button.onclick = async () =&gt; {\n                task2Button.disabled = true; // 禁用按钮以防重复点击\n                console.log(&quot;任务 2: 商品数据导出&quot;);\n\n                // 点击配置指标按钮\n                const configButtonSelector = 'div.mx-12 button';\n                simulateClick(configButtonSelector);\n\n                // 等待框关闭\n                await waitForModalToClose();\n\n                // 点击导出按钮\n                const exportButtonSelector = 'div.flex.gap-8 button';\n                simulateClick(exportButtonSelector);\n\n                task2Button.disabled = false; // 点击完成后启用按钮\n            };\n\n            // 创建按钮容器并插入按钮\n            const buttonContainer = document.createElement(&quot;div&quot;);\n            buttonContainer.style.display = &quot;flex&quot;;\n            buttonContainer.style.justifyContent = &quot;center&quot;;\n            buttonContainer.style.alignItems = &quot;center&quot;;\n            buttonContainer.style.height = &quot;50px&quot;; // 调整高度以便更好地居中\n            buttonContainer.appendChild(task1Button);\n            buttonContainer.appendChild(task2Button);\n\n            // 在 pageMain 元素的上方插入按钮容器\n            pageMain.parentNode.insertBefore(buttonContainer, pageMain);\n        }\n    });\n}\n\nfunction styleButton(button) {\n    button.style.padding = &quot;10px 20px&quot;;\n    button.style.margin = &quot;5px&quot;;\n    button.style.fontSize = &quot;16px&quot;;\n    button.style.backgroundColor = &quot;#4CAF50&quot;;\n    button.style.color = &quot;white&quot;;\n    button.style.border = &quot;none&quot;;\n    button.style.borderRadius = &quot;5px&quot;;\n    button.style.cursor = &quot;pointer&quot;;\n    button.style.boxShadow = &quot;0px 4px 6px rgba(0, 0, 0, 0.1)&quot;;\n    button.style.transition = &quot;background-color 0.3s&quot;;\n    button.onmouseover = function() {\n        button.style.backgroundColor = &quot;#45a049&quot;;\n    };\n    button.onmouseout = function() {\n        button.style.backgroundColor = &quot;#4CAF50&quot;;\n    };\n}\n\n// 调用 main 函数以执行代码\nmain();\n\n</code></pre>\n"}},4307:function(n){n.exports={attributes:{title:"从CNN循环神经网络到Transformer模型",date:"2024-11-01T00:00:00.000Z",summary:"在尝试的学习CV技术",coverImage:"11-19-7.jpg",pinned:!1},html:"<p>CNN最初就是从线性层变过来的一个模型结构，只是多加了一个隐藏变量。</p>\n<h2>RNN模型</h2>\n<p>$$\nh_t = \\sigma_h(W_{xh} x_t + W_{hh} h_{t-1} + b_h)\n$$</p>\n<p>$$\ny_t=\\sigma_y(W _{hy}h_t+b _y)\n$$</p>\n<p>$$\nf_t=\\sigma(W_f⋅concat[h _{t−1},x _t]+b _f)\n$$</p>\n<p>当前层的输出会</p>\n"}},2044:function(n){n.exports={attributes:{title:"文本情感分析模型",date:"2024-11-21T00:00:00.000Z",summary:"这是整理代码的文章",coverImage:"11-19-6.jpg",pinned:!0},html:"<h1>这是一个置顶文章测试</h1>\n"}},8277:function(n){n.exports={attributes:{title:"测试文章",date:"2024-11-1",summary:"这是我的第一篇博客文章",coverImage:"post1/2.png",pinned:!1},html:'<h1>欢迎来到我的博客</h1>\n<p>这是一篇测试博客，如果它能正常显示成功就代表它运行正常了。</p>\n<h2>数学公式示例</h2>\n<p>$\nE = mc^2\n$</p>\n<h2>代码块示例</h2>\n<pre><code class="language-python">def hello_world():\n    print(&quot;Hello, World!&quot;)\n</code></pre>\n<h2>图片示例</h2>\n<p><img src="posts/images/1.png" alt="本地图片"></p>\n'}},8642:function(n,t,e){"use strict";var o=e(5130),a=e(6768);const r={class:"app"};function i(n,t,e,o,i,s){const l=(0,a.g2)("blog-header"),u=(0,a.g2)("router-view"),d=(0,a.g2)("blog-footer");return(0,a.uX)(),(0,a.CE)("div",r,[(0,a.bF)(l),(0,a.bF)(u),(0,a.bF)(d)])}var s=e(4232);const l={class:"header-content"},u={class:"header-main"},d={class:"site-nav"};function c(n,t,e,o,r,i){const c=(0,a.g2)("router-link");return(0,a.uX)(),(0,a.CE)("header",{class:(0,s.C4)(["header",{"header-scrolled":r.isScrolled}])},[(0,a.Lk)("div",l,[(0,a.Lk)("div",u,[t[3]||(t[3]=(0,a.Lk)("h1",{class:"site-title"},"星云茶聚",-1)),(0,a.Lk)("nav",d,[(0,a.bF)(c,{to:"/",class:"nav-link"},{default:(0,a.k6)((()=>t[0]||(t[0]=[(0,a.eW)("首页")]))),_:1}),(0,a.bF)(c,{to:"/archive",class:"nav-link"},{default:(0,a.k6)((()=>t[1]||(t[1]=[(0,a.eW)("归档")]))),_:1}),(0,a.bF)(c,{to:"/about",class:"nav-link"},{default:(0,a.k6)((()=>t[2]||(t[2]=[(0,a.eW)("关于")]))),_:1})])])])],2)}var p={name:"BlogHeader",data(){return{isScrolled:!1,lastScrollTop:0,headerHeight:0,scrollThreshold:50,scrollTimer:null}},mounted(){this.headerHeight=this.$el.offsetHeight,window.addEventListener("scroll",this.handleScroll,{passive:!0}),document.body.style.paddingTop=`${this.headerHeight+40}px`},beforeUnmount(){window.removeEventListener("scroll",this.handleScroll),document.body.style.paddingTop="0",this.scrollTimer&&clearTimeout(this.scrollTimer)},methods:{handleScroll(){this.scrollTimer&&cancelAnimationFrame(this.scrollTimer),this.scrollTimer=requestAnimationFrame((()=>{const n=window.scrollY;if(n<=this.scrollThreshold)return void(this.isScrolled=!1);const t=n-this.lastScrollTop;Math.abs(t)>this.scrollThreshold&&(this.isScrolled=!(t>0),this.lastScrollTop=n)}))}}},m=e(1241);const _=(0,m.A)(p,[["render",c],["__scopeId","data-v-1e9d4ba0"]]);var g=_;const q={class:"footer"};function f(n,t,e,o,r,i){return(0,a.uX)(),(0,a.CE)("footer",q,[(0,a.Lk)("p",null,"© "+(0,s.v_)(i.currentYear)+" 星云茶聚. All rights reserved.",1),t[0]||(t[0]=(0,a.Lk)("div",{class:"social-links"},[(0,a.Lk)("a",{href:"#"},"微博"),(0,a.eW)(" | "),(0,a.Lk)("a",{href:"https://github.com/SUXUING-star/SUXUING-star.github.io"},"GitHub"),(0,a.eW)(" | "),(0,a.Lk)("a",{href:"#"},"LinkedIn")],-1))])}var h={name:"BlogFooter",computed:{currentYear(){return(new Date).getFullYear()}}};const v=(0,m.A)(h,[["render",f]]);var b=v,y={name:"App",components:{BlogHeader:g,BlogFooter:b}};const x=(0,m.A)(y,[["render",i]]);var w=x,k=e(1387);const T={class:"home-page"},L={key:0,class:"pinned-posts"},S={class:"posts-grid"},z={class:"regular-posts"},C={key:0,class:"section-title"},E={class:"posts-grid"},M={key:1,class:"pagination"},R=["disabled"],B={class:"pagination-numbers"},P=["onClick"],D=["disabled"];function A(n,t,e,o,r,i){const l=(0,a.g2)("blog-post");return(0,a.uX)(),(0,a.CE)("div",T,[t[3]||(t[3]=(0,a.Lk)("section",{class:"banner"},[(0,a.Lk)("div",{class:"banner-content"},[(0,a.Lk)("h2",null,"欢迎来到这个神奇的地方！"),(0,a.Lk)("p",null,"记录成长，探索未知的世界。")])],-1)),o.pinnedPosts.length?((0,a.uX)(),(0,a.CE)("section",L,[t[2]||(t[2]=(0,a.Lk)("h3",{class:"section-title"},"📌 置顶文章",-1)),(0,a.Lk)("div",S,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(o.pinnedPosts,(n=>((0,a.uX)(),(0,a.Wv)(l,{key:n.id,post:n,onClick:t=>o.viewPost(n.id)},null,8,["post","onClick"])))),128))])])):(0,a.Q3)("",!0),(0,a.Lk)("section",z,[o.pinnedPosts.length?((0,a.uX)(),(0,a.CE)("h3",C,"最新文章")):(0,a.Q3)("",!0),(0,a.Lk)("div",E,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(o.paginatedRegularPosts,(n=>((0,a.uX)(),(0,a.Wv)(l,{key:n.id,post:n,onClick:t=>o.viewPost(n.id)},null,8,["post","onClick"])))),128))])]),o.totalPages>1?((0,a.uX)(),(0,a.CE)("div",M,[(0,a.Lk)("button",{class:"pagination-btn",disabled:1===o.currentPage,onClick:t[0]||(t[0]=n=>o.changePage(o.currentPage-1))}," ← 上一页 ",8,R),(0,a.Lk)("div",B,[((0,a.uX)(!0),(0,a.CE)(a.FK,null,(0,a.pI)(o.displayedPages,(n=>((0,a.uX)(),(0,a.CE)("button",{key:n,class:(0,s.C4)(["page-number",{active:n===o.currentPage}]),onClick:t=>o.changePage(n)},(0,s.v_)(n),11,P)))),128))]),(0,a.Lk)("button",{class:"pagination-btn",disabled:o.currentPage===o.totalPages,onClick:t[1]||(t[1]=n=>o.changePage(o.currentPage+1))}," 下一页 → ",8,D)])):(0,a.Q3)("",!0)])}e(4114),e(8992),e(4520);var j=e(4249),U=e(144);const O={key:0,class:"pin-badge"},F={class:"post-image"},N=["src","alt"],I={key:1,src:"/placeholder-image.gif",alt:"Placeholder Image",loading:"lazy"},$={class:"post-content"},W={class:"post-title"},G={class:"post-meta"},H={class:"post-date"},X={class:"post-summary"};function Y(n,t,e,o,r,i){return(0,a.uX)(),(0,a.CE)("article",{class:(0,s.C4)(["blog-post-card",{"blog-post-pinned":e.post.pinned}]),onClick:t[1]||(t[1]=t=>n.$emit("click"))},[e.post.pinned?((0,a.uX)(),(0,a.CE)("div",O,t[2]||(t[2]=[(0,a.Lk)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[(0,a.Lk)("line",{x1:"12",y1:"17",x2:"12",y2:"22"}),(0,a.Lk)("path",{d:"M5 17h14v-1.76a2 2 0 0 0-1.11-1.79l-1.78-.9A2 2 0 0 1 15 10.76V6h1a2 2 0 0 0 0-4H8a2 2 0 0 0 0 4h1v4.76a2 2 0 0 1-1.11 1.79l-1.78.9A2 2 0 0 0 5 15.24Z"})],-1),(0,a.Lk)("span",null,"置顶",-1)]))):(0,a.Q3)("",!0),(0,a.Lk)("div",F,[e.post.coverImage?((0,a.uX)(),(0,a.CE)("img",{key:0,src:e.post.coverImage,alt:e.post.title,onError:t[0]||(t[0]=(...n)=>i.handleImageError&&i.handleImageError(...n)),loading:"lazy"},null,40,N)):((0,a.uX)(),(0,a.CE)("img",I))]),(0,a.Lk)("div",$,[(0,a.Lk)("h2",W,(0,s.v_)(e.post.title),1),(0,a.Lk)("div",G,[(0,a.Lk)("span",H,(0,s.v_)(e.post.date),1)]),(0,a.Lk)("p",X,(0,s.v_)(e.post.summary),1),t[3]||(t[3]=(0,a.Lk)("div",{class:"post-footer"},[(0,a.Lk)("span",{class:"read-more"},"阅读全文 →")],-1))])],2)}var J={name:"BlogPostItem",props:{post:{type:Object,required:!0,default:()=>({coverImage:"",summary:""}),validator:function(n){return n&&"string"===typeof n.title&&"string"===typeof n.date&&(!("pinned"in n)||"boolean"===typeof n.pinned)&&("string"===typeof n.coverImage||void 0===n.coverImage)&&"string"===typeof n.summary}}},methods:{handleImageError(n){n.target.src="/placeholder-image.gif"}}};const V=(0,m.A)(J,[["render",Y],["__scopeId","data-v-7a8ffa7a"]]);var Z=V,K={name:"HomePage",components:{BlogPost:Z},setup(){const n=(0,j.Pj)(),t=(0,k.rd)(),e=6,o=(0,U.KR)(1),r=(0,a.EW)((()=>(console.log("Posts:",n.state.posts),n.state.posts))),i=(0,a.EW)((()=>r.value.filter((n=>!0===n.frontmatter?.pinned||!0===n.pinned)))),s=(0,a.EW)((()=>r.value.filter((n=>!0!==n.pinned)))),l=(0,a.EW)((()=>Math.ceil(s.value.length/e))),u=(0,a.EW)((()=>{const n=(o.value-1)*e,t=n+e;return s.value.slice(n,t)})),d=(0,a.EW)((()=>{const n=l.value,t=o.value,e=[];if(n<=5)for(let o=1;o<=n;o++)e.push(o);else if(t<=3)for(let o=1;o<=5;o++)e.push(o);else if(t>=n-2)for(let o=n-4;o<=n;o++)e.push(o);else for(let o=t-2;o<=t+2;o++)e.push(o);return e})),c=n=>{o.value=n,window.scrollTo({top:0,behavior:"smooth"})},p=n=>{t.push(`/post/${n}`)};return{pinnedPosts:i,paginatedRegularPosts:u,currentPage:o,totalPages:l,displayedPages:d,changePage:c,viewPost:p}}};const Q=(0,m.A)(K,[["render",A],["__scopeId","data-v-6ccd4249"]]);var nn=Q;const tn=[{path:"/",name:"Home",component:nn,beforeEnter:(n,t,o)=>{const a=[e.e(594).then(e.bind(e,3431)),e.e(503).then(e.bind(e,9294))];Promise.all(a).catch((()=>{})),o()}},{path:"/about",name:"About",component:()=>e.e(594).then(e.bind(e,3431)),meta:{keepAlive:!0}},{path:"/post/:id",name:"PostDetail",component:()=>Promise.all([e.e(137),e.e(205)]).then(e.bind(e,4193)),props:!0},{path:"/archive",name:"Archive",component:()=>e.e(503).then(e.bind(e,9294)),meta:{keepAlive:!0}}],en=(0,k.aE)({history:(0,k.Bt)(),routes:tn,scrollBehavior(n,t,e){return e||{top:0}}});en.beforeEach(((n,t,e)=>{e()}));var on=en;e(2577),e(1454);const an=e(5344);function rn(n){const t=new Date(n);return t.toLocaleDateString("zh-CN",{year:"numeric",month:"long",day:"numeric"})}function sn(n){if(!n)return"";if(n.startsWith("http"))return n;try{return e(1751)(`./${n}`)}catch(t){return console.warn(`Image not found: ${n}`),""}}function ln(n){return n.replace(/!\[(.*?)\]\((.*?)\)/g,((n,t,o)=>{if(o.startsWith("http"))return n;try{const n=e(1751)(`./${o}`);return`![${t}](${n})`}catch(a){return console.warn(`Image not found in markdown: ${o}`),n}}))}function un(){return an.keys().map(((n,t)=>{const e=n.replace(/^\.\//,"").replace(/\.md$/,""),{attributes:o,html:a}=an(n);return{id:t+1,slug:e,title:o.title,date:rn(o.date),summary:o.summary,coverImage:sn(o.coverImage),pinned:o.pinned||!1,content:ln(a)}}))}const dn=(0,j.y$)({state(){return{posts:un()}},getters:{getPostById:n=>t=>n.posts.find((n=>n.id===parseInt(t))),getPostBySlug:n=>t=>n.posts.find((n=>n.slug===t)),getAllPosts:n=>n.posts},mutations:{UPDATE_POST(n,{id:t,post:e}){const o=n.posts.findIndex((n=>n.id===t));-1!==o&&(n.posts[o]={...n.posts[o],...e})}},actions:{updatePost({commit:n},t){n("UPDATE_POST",t)}}});var cn=dn;e(9351);const pn=(0,o.Ef)(w);pn.use(cn),pn.use(on),pn.mount("#app")},1751:function(n,t,e){var o={"./11-19-1.jpg":5567,"./11-19-2.jpg":5248,"./11-19-3.jpg":3341,"./11-19-4.jpg":646,"./11-19-5.jpg":5411,"./11-19-6.jpg":9188,"./11-19-7.jpg":2785,"./11-19-8.jpg":6058,"./11-19-9.jpg":4343,"./3.png":4415,"./post1/2.png":380,"./post2/post2-1.png":9743};function a(n){var t=r(n);return e(t)}function r(n){if(!e.o(o,n)){var t=new Error("Cannot find module '"+n+"'");throw t.code="MODULE_NOT_FOUND",t}return o[n]}a.keys=function(){return Object.keys(o)},a.resolve=r,n.exports=a,a.id=1751},5344:function(n,t,e){var o={"./post2.md":3937,"./post3.md":9317,"./post4.md":2879,"./post5.md":3352,"./post6.md":277,"./post7.md":8774,"./post8.md":4307,"./post9.md":2044,"./welcome.md":8277};function a(n){var t=r(n);return e(t)}function r(n){if(!e.o(o,n)){var t=new Error("Cannot find module '"+n+"'");throw t.code="MODULE_NOT_FOUND",t}return o[n]}a.keys=function(){return Object.keys(o)},a.resolve=r,n.exports=a,a.id=5344},5567:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-1.6f7c8cba.jpg"},5248:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-2.654c3360.jpg"},3341:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-3.6679b0ed.jpg"},646:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-4.c3db53a2.jpg"},5411:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-5.a70f60f8.jpg"},9188:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-6.b9cac217.jpg"},2785:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-7.91a6753e.jpg"},6058:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-8.178cf2e7.jpg"},4343:function(n,t,e){"use strict";n.exports=e.p+"img/11-19-9.e866b537.jpg"},4415:function(n,t,e){"use strict";n.exports=e.p+"img/3.1cf9cafa.png"},380:function(n,t,e){"use strict";n.exports=e.p+"img/2.82a2bed7.png"},9743:function(n,t,e){"use strict";n.exports=e.p+"img/post2-1.e41820ee.png"}},t={};function e(o){var a=t[o];if(void 0!==a)return a.exports;var r=t[o]={exports:{}};return n[o].call(r.exports,r,r.exports,e),r.exports}e.m=n,function(){var n=[];e.O=function(t,o,a,r){if(!o){var i=1/0;for(d=0;d<n.length;d++){o=n[d][0],a=n[d][1],r=n[d][2];for(var s=!0,l=0;l<o.length;l++)(!1&r||i>=r)&&Object.keys(e.O).every((function(n){return e.O[n](o[l])}))?o.splice(l--,1):(s=!1,r<i&&(i=r));if(s){n.splice(d--,1);var u=a();void 0!==u&&(t=u)}}return t}r=r||0;for(var d=n.length;d>0&&n[d-1][2]>r;d--)n[d]=n[d-1];n[d]=[o,a,r]}}(),function(){e.n=function(n){var t=n&&n.__esModule?function(){return n["default"]}:function(){return n};return e.d(t,{a:t}),t}}(),function(){e.d=function(n,t){for(var o in t)e.o(t,o)&&!e.o(n,o)&&Object.defineProperty(n,o,{enumerable:!0,get:t[o]})}}(),function(){e.f={},e.e=function(n){return Promise.all(Object.keys(e.f).reduce((function(t,o){return e.f[o](n,t),t}),[]))}}(),function(){e.u=function(n){return"js/"+({205:"post",503:"archive",594:"about"}[n]||n)+"."+{137:"5b3b94a8",205:"613e6bae",503:"29cb7fcf",594:"6f7f4a2a"}[n]+".js"}}(),function(){e.miniCssF=function(n){return"css/post.d68cf366.css"}}(),function(){e.g=function(){if("object"===typeof globalThis)return globalThis;try{return this||new Function("return this")()}catch(n){if("object"===typeof window)return window}}()}(),function(){e.o=function(n,t){return Object.prototype.hasOwnProperty.call(n,t)}}(),function(){var n={},t="xingyunchaju:";e.l=function(o,a,r,i){if(n[o])n[o].push(a);else{var s,l;if(void 0!==r)for(var u=document.getElementsByTagName("script"),d=0;d<u.length;d++){var c=u[d];if(c.getAttribute("src")==o||c.getAttribute("data-webpack")==t+r){s=c;break}}s||(l=!0,s=document.createElement("script"),s.charset="utf-8",s.timeout=120,e.nc&&s.setAttribute("nonce",e.nc),s.setAttribute("data-webpack",t+r),s.src=o),n[o]=[a];var p=function(t,e){s.onerror=s.onload=null,clearTimeout(m);var a=n[o];if(delete n[o],s.parentNode&&s.parentNode.removeChild(s),a&&a.forEach((function(n){return n(e)})),t)return t(e)},m=setTimeout(p.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=p.bind(null,s.onerror),s.onload=p.bind(null,s.onload),l&&document.head.appendChild(s)}}}(),function(){e.r=function(n){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(n,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(n,"__esModule",{value:!0})}}(),function(){e.p="/SUXUING-star.github.io/"}(),function(){if("undefined"!==typeof document){var n=function(n,t,o,a,r){var i=document.createElement("link");i.rel="stylesheet",i.type="text/css",e.nc&&(i.nonce=e.nc);var s=function(e){if(i.onerror=i.onload=null,"load"===e.type)a();else{var o=e&&e.type,s=e&&e.target&&e.target.href||t,l=new Error("Loading CSS chunk "+n+" failed.\n("+o+": "+s+")");l.name="ChunkLoadError",l.code="CSS_CHUNK_LOAD_FAILED",l.type=o,l.request=s,i.parentNode&&i.parentNode.removeChild(i),r(l)}};return i.onerror=i.onload=s,i.href=t,o?o.parentNode.insertBefore(i,o.nextSibling):document.head.appendChild(i),i},t=function(n,t){for(var e=document.getElementsByTagName("link"),o=0;o<e.length;o++){var a=e[o],r=a.getAttribute("data-href")||a.getAttribute("href");if("stylesheet"===a.rel&&(r===n||r===t))return a}var i=document.getElementsByTagName("style");for(o=0;o<i.length;o++){a=i[o],r=a.getAttribute("data-href");if(r===n||r===t)return a}},o=function(o){return new Promise((function(a,r){var i=e.miniCssF(o),s=e.p+i;if(t(i,s))return a();n(o,s,null,a,r)}))},a={524:0};e.f.miniCss=function(n,t){var e={205:1};a[n]?t.push(a[n]):0!==a[n]&&e[n]&&t.push(a[n]=o(n).then((function(){a[n]=0}),(function(t){throw delete a[n],t})))}}}(),function(){var n={524:0};e.f.j=function(t,o){var a=e.o(n,t)?n[t]:void 0;if(0!==a)if(a)o.push(a[2]);else{var r=new Promise((function(e,o){a=n[t]=[e,o]}));o.push(a[2]=r);var i=e.p+e.u(t),s=new Error,l=function(o){if(e.o(n,t)&&(a=n[t],0!==a&&(n[t]=void 0),a)){var r=o&&("load"===o.type?"missing":o.type),i=o&&o.target&&o.target.src;s.message="Loading chunk "+t+" failed.\n("+r+": "+i+")",s.name="ChunkLoadError",s.type=r,s.request=i,a[1](s)}};e.l(i,l,"chunk-"+t,t)}},e.O.j=function(t){return 0===n[t]};var t=function(t,o){var a,r,i=o[0],s=o[1],l=o[2],u=0;if(i.some((function(t){return 0!==n[t]}))){for(a in s)e.o(s,a)&&(e.m[a]=s[a]);if(l)var d=l(e)}for(t&&t(o);u<i.length;u++)r=i[u],e.o(n,r)&&n[r]&&n[r][0](),n[r]=0;return e.O(d)},o=self["webpackChunkxingyunchaju"]=self["webpackChunkxingyunchaju"]||[];o.forEach(t.bind(null,0)),o.push=t.bind(null,o.push.bind(o))}();var o=e.O(void 0,[683,501,849,823],(function(){return e(8642)}));o=e.O(o)})();
//# sourceMappingURL=app.a390ecbb.js.map